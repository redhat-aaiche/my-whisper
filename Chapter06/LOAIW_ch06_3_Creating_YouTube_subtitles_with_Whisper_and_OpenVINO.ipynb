{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geetings to : https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKIoAhqZXkbt"
   },
   "source": [
    "# Learn OpenAI Whisper - Chapter 6\n",
    "## Notebook 3: Video Subtitle Generation using Whisper and OpenVINO™\n",
    "\n",
    "\n",
    "In this advanced tutorial, we will leverage the power of OpenAI's Whisper model in conjunction with OpenVINO toolkit to automatically generate subtitles for a sample video. The process will be broken down into the following key steps:\n",
    "\n",
    "1. Obtaining the pre-trained Whisper model\n",
    "2. Setting up the PyTorch model pipeline\n",
    "3. Transforming the model into OpenVINO Intermediate Representation (IR) format using the model conversion API\n",
    "4. Executing the Whisper pipeline with the converted OpenVINO models to generate the subtitles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Htz6uLXkby"
   },
   "source": [
    "## Setting Up the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rSRffHmXkby"
   },
   "source": [
    "We start by importing a helper Python utility module called utils.py from our GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5IxkRGGXZHwG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 11:48:23 URL:https://raw.githubusercontent.com/redhat-aaiche/my-whisper/refs/heads/main/Chapter06/utils.py [11251/11251] -> \"utils.py\" [1]\n"
     ]
    }
   ],
   "source": [
    "!wget -nv \"https://raw.githubusercontent.com/redhat-aaiche/my-whisper/refs/heads/main/Chapter06/utils.py\" -O utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8NCIh7jffKd"
   },
   "source": [
    "Next, we install critical software dependencies to enable working with AI models and speech data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aa execute the following cell TWICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9AJOshJXkby",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q cohere openai tiktoken\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "%pip install -q \"python-ffmpeg<=1.0.16\" moviepy transformers --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"git+https://github.com/garywu007/pytube.git\"\n",
    "%pip install -q gradio\n",
    "%pip install -q \"openai-whisper==20231117\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm1KBIgAXkb0"
   },
   "source": [
    "## Initializing the Whisper Model\n",
    "\n",
    "OpenAI's Whisper is a powerful Transformer-based encoder-decoder model, also known as a sequence-to-sequence model, designed for speech recognition tasks. It operates by mapping a sequence of audio spectrogram features to a corresponding sequence of text tokens. The process can be broken down into three main steps:\n",
    "\n",
    "1. **Feature Extraction**: The raw audio inputs are first converted into a log-Mel spectrogram representation using a feature extractor module.\n",
    "\n",
    "2. **Encoding**: The Transformer encoder then processes the spectrogram, generating a sequence of hidden states that capture the essential information from the audio input.\n",
    "\n",
    "3. **Decoding**: Finally, the decoder autoregressively predicts the text tokens, conditioned on both the previously generated tokens and the encoder's hidden states.\n",
    "\n",
    "The architecture of the Whisper model is illustrated in the diagram below:\n",
    "\n",
    "![whisper_architecture.svg](https://user-images.githubusercontent.com/29454499/204536571-8f6d8d77-5fbd-4c6d-8e29-14e734837860.svg)\n",
    "\n",
    "*Source: https://openai.com/research/whisper*\n",
    "\n",
    "By leveraging this powerful architecture, Whisper achieves state-of-the-art performance on various speech recognition benchmarks, making it an ideal choice for our subtitle generation task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXNNEelqXkb0"
   },
   "source": [
    "The creators of Whisper have trained several models with varying sizes and capabilities to cater to different use cases and resource constraints. For the purpose of this tutorial, we will be using the `base` model, which offers a good balance between performance and efficiency. However, it's important to note that the steps and techniques demonstrated in this notebook can be easily applied to other models within the Whisper family, allowing you to experiment with different configurations and find the one that best suits your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pX-7rKhCXkb0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8137e8e0921446fb8babef7c136cadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=3, options=('tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'm…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whisper import _MODELS\n",
    "import ipywidgets as widgets\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=list(_MODELS),\n",
    "    value='base',\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/whisper/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/whisper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HTBnlzWzXkb1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:06<00:00, 21.6MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# model = whisper.load_model(model_id.value)\n",
    "model = whisper.load_model(model_id.value, \"cpu\")\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 139M\n",
      "drwxr-xr-x. 2 1000800000 root   21 Jan 13 11:40 .\n",
      "drwxr-xr-x. 8 1000800000 root   94 Jan 13 11:40 ..\n",
      "-rw-r--r--. 1 1000800000 root 139M Jan 13 11:41 base.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/whisper/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivt20ZOuXkb2"
   },
   "source": [
    "### Converting the Model to OpenVINO Intermediate Representation (IR) Format\n",
    "\n",
    "To achieve optimal performance and efficiency with the OpenVINO toolkit, it is highly recommended to convert the Whisper model into the OpenVINO-specific Intermediate Representation (IR) format. This process requires two key components:\n",
    "\n",
    "1. An initialized model object\n",
    "2. Sample input data for shape inference\n",
    "\n",
    "We will leverage the `ov.convert_model` function provided by OpenVINO to perform the model conversion. This function takes the initialized model object and sample inputs as arguments and returns an OpenVINO-compatible model that is ready to be loaded onto the target device for inference.\n",
    "\n",
    "Once the conversion is complete, we can save the OpenVINO model to disk using the `ov.save_model` function. This allows us to reuse the converted model in future sessions without the need to repeat the conversion process, saving valuable time and resources.\n",
    "\n",
    "By converting the Whisper model to OpenVINO IR format, we can take full advantage of the performance optimizations and hardware acceleration capabilities offered by the OpenVINO toolkit, ensuring efficient and high-quality subtitle generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHqv0rcDXkb2"
   },
   "source": [
    "### Converting the Whisper Encoder to OpenVINO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "iqoRzYnAXkb2",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WHISPER_ENCODER_OV = Path(f\"whisper_{model_id.value}_encoder.xml\")\n",
    "WHISPER_DECODER_OV = Path(f\"whisper_{model_id.value}_decoder.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuGuzFCjsEyT"
   },
   "source": [
    "\n",
    "An example input is created using a tensor of zeros. The ov.convert_model function is then used to convert the encoder model to OpenVINO's IR format. The converted model is saved to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hbtJtI0CXkb2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/whisper/model.py:166: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "mel = torch.zeros((1, 80 if 'v3' not in model_id.value else 128, 3000))\n",
    "audio_features = model.encoder(mel)\n",
    "if not WHISPER_ENCODER_OV.exists():\n",
    "    encoder_model = ov.convert_model(model.encoder, example_input=mel)\n",
    "    ov.save_model(encoder_model, WHISPER_ENCODER_OV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xug44Bx7Xkb3"
   },
   "source": [
    " ### Converting the Whisper Decoder to OpenVINO IR\n",
    "\n",
    "The Whisper decoder employs a technique called attention caching to reduce computational complexity and improve efficiency. This involves storing the key and value projections from previous steps in the attention modules, which can then be reused in subsequent computations. However, to ensure accurate tracing and conversion of the decoder to OpenVINO IR format, we need to modify this caching mechanism.\n",
    "\n",
    "In the following code cells, we will define custom forward functions for the decoder's attention modules and residual blocks. These modified functions will explicitly handle the caching and retrieval of key and value projections, making the caching process more transparent and traceable.\n",
    "\n",
    "By adapting the decoder's architecture to be more compatible with the OpenVINO conversion process, we can successfully convert the Whisper decoder to OpenVINO IR format, enabling us to leverage the performance benefits of the OpenVINO toolkit while maintaining the decoder's functionality and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yHZ-eQX4Xkb3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def attention_forward(\n",
    "        attention_module,\n",
    "        x: torch.Tensor,\n",
    "        xa: Optional[torch.Tensor] = None,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Override for forward method of decoder attention module with storing cache values explicitly.\n",
    "    Parameters:\n",
    "      attention_module: current attention module\n",
    "      x: input token ids.\n",
    "      xa: input audio features (Optional).\n",
    "      mask: mask for applying attention (Optional).\n",
    "      kv_cache: dictionary with cached key values for attention modules.\n",
    "      idx: idx for search in kv_cache.\n",
    "    Returns:\n",
    "      attention module output tensor\n",
    "      updated kv_cache\n",
    "    \"\"\"\n",
    "    q = attention_module.query(x)\n",
    "\n",
    "    if xa is None:\n",
    "        # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "        # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "        k = attention_module.key(x)\n",
    "        v = attention_module.value(x)\n",
    "        if kv_cache is not None:\n",
    "            k = torch.cat((kv_cache[0], k), dim=1)\n",
    "            v = torch.cat((kv_cache[1], v), dim=1)\n",
    "        kv_cache_new = (k, v)\n",
    "    else:\n",
    "        # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "        k = attention_module.key(xa)\n",
    "        v = attention_module.value(xa)\n",
    "        kv_cache_new = (None, None)\n",
    "\n",
    "    wv, qk = attention_module.qkv_attention(q, k, v, mask)\n",
    "    return attention_module.out(wv), kv_cache_new\n",
    "\n",
    "\n",
    "def block_forward(\n",
    "    residual_block,\n",
    "    x: torch.Tensor,\n",
    "    xa: Optional[torch.Tensor] = None,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Override for residual block forward method for providing kv_cache to attention module.\n",
    "      Parameters:\n",
    "        residual_block: current residual block.\n",
    "        x: input token_ids.\n",
    "        xa: input audio features (Optional).\n",
    "        mask: attention mask (Optional).\n",
    "        kv_cache: cache for storing attention key values.\n",
    "      Returns:\n",
    "        x: residual block output\n",
    "        kv_cache: updated kv_cache\n",
    "\n",
    "    \"\"\"\n",
    "    x0, kv_cache = residual_block.attn(residual_block.attn_ln(\n",
    "        x), mask=mask, kv_cache=kv_cache)\n",
    "    x = x + x0\n",
    "    if residual_block.cross_attn:\n",
    "        x1, _ = residual_block.cross_attn(\n",
    "            residual_block.cross_attn_ln(x), xa)\n",
    "        x = x + x1\n",
    "    x = x + residual_block.mlp(residual_block.mlp_ln(x))\n",
    "    return x, kv_cache\n",
    "\n",
    "\n",
    "\n",
    "# update forward functions\n",
    "for idx, block in enumerate(model.decoder.blocks):\n",
    "    block.forward = partial(block_forward, block)\n",
    "    block.attn.forward = partial(attention_forward, block.attn)\n",
    "    if block.cross_attn:\n",
    "        block.cross_attn.forward = partial(attention_forward, block.cross_attn)\n",
    "\n",
    "\n",
    "def decoder_forward(decoder, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor]]] = None):\n",
    "    \"\"\"\n",
    "    Override for decoder forward method.\n",
    "    Parameters:\n",
    "      x: torch.LongTensor, shape = (batch_size, <= n_ctx) the text tokens\n",
    "      xa: torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n",
    "           the encoded audio features to be attended on\n",
    "      kv_cache: Dict[str, torch.Tensor], attention modules hidden states cache from previous steps\n",
    "    \"\"\"\n",
    "    if kv_cache is not None:\n",
    "        offset = kv_cache[0][0].shape[1]\n",
    "    else:\n",
    "        offset = 0\n",
    "        kv_cache = [None for _ in range(len(decoder.blocks))]\n",
    "    x = decoder.token_embedding(\n",
    "        x) + decoder.positional_embedding[offset: offset + x.shape[-1]]\n",
    "    x = x.to(xa.dtype)\n",
    "    kv_cache_upd = []\n",
    "\n",
    "    for block, kv_block_cache in zip(decoder.blocks, kv_cache):\n",
    "        x, kv_block_cache_upd = block(x, xa, mask=decoder.mask, kv_cache=kv_block_cache)\n",
    "        kv_cache_upd.append(tuple(kv_block_cache_upd))\n",
    "\n",
    "    x = decoder.ln(x)\n",
    "    logits = (\n",
    "        x @ torch.transpose(decoder.token_embedding.weight.to(x.dtype), 1, 0)).float()\n",
    "\n",
    "    return logits, tuple(kv_cache_upd)\n",
    "\n",
    "\n",
    "\n",
    "# override decoder forward\n",
    "model.decoder.forward = partial(decoder_forward, model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-jcuygEqXkb3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torch/jit/_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.ones((5, 3), dtype=torch.int64)\n",
    "logits, kv_cache = model.decoder(tokens, audio_features, kv_cache=None)\n",
    "\n",
    "tokens = torch.ones((5, 1), dtype=torch.int64)\n",
    "\n",
    "if not WHISPER_DECODER_OV.exists():\n",
    "    decoder_model = ov.convert_model(model.decoder, example_input=(tokens, audio_features, kv_cache))\n",
    "    ov.save_model(decoder_model, WHISPER_DECODER_OV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFrawf8fXkb3"
   },
   "source": [
    "The decoder model autoregressively predicts the next token guided by encoder hidden states and previously predicted sequence. This means that the shape of inputs which depends on the previous step (inputs for tokens and attention hidden states from previous step) are dynamic. For efficient utilization of memory, you define an upper bound for dynamic input shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbzyqfo9Xkb3"
   },
   "source": [
    "### Preparing the Inference Pipeline\n",
    "\n",
    "The image below illustrates the pipeline of video transcribing using the Whisper model.\n",
    "\n",
    "![ch06_diagram01.png](https://raw.githubusercontent.com/PacktPublishing/Learn-OpenAI-Whisper/main/Chapter06/ch06_diagram01.png)\n",
    "\n",
    "To run the PyTorch Whisper model, we just need to call the `model.transcribe(audio, **parameters)` function. We will try to reuse original model pipeline for audio transcribing after replacing the original models with OpenVINO IR versions.\n",
    "\n",
    "In the original PyTorch implementation of Whisper, running the transcription pipeline is as simple as calling the `model.transcribe(audio, **parameters)` function, which handles all the necessary steps internally.\n",
    "\n",
    "To leverage the benefits of the OpenVINO toolkit, we will modify this pipeline by replacing the original PyTorch models with their OpenVINO IR counterparts. By doing so, we can take advantage of the performance optimizations and hardware acceleration capabilities offered by OpenVINO while maintaining the overall structure and functionality of the transcription pipeline.\n",
    "\n",
    "In the following sections, we will dive deeper into each step of the pipeline and demonstrate how to integrate the OpenVINO models seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5TTPkpuXkb4"
   },
   "source": [
    "### Selecting the Inference Device\n",
    "\n",
    "One of the key advantages of the OpenVINO toolkit is its ability to optimize and run inference on a wide range of hardware devices, including CPUs, GPUs, and specialized accelerators. To harness this flexibility, we need to specify the target device on which we want to execute the inference pipeline.\n",
    "\n",
    "In the code cell below, you will find a dropdown menu that allows you to select the desired inference device. The available options are dynamically populated based on the devices supported by your system and the installed OpenVINO runtime.\n",
    "\n",
    "Simply choose the appropriate device from the dropdown list, considering factors such as performance, power consumption, and availability. OpenVINO will then optimize the converted models and execute the inference pipeline on the selected device, ensuring the best possible performance and efficiency.\n",
    "\n",
    "By default, the \"AUTO\" option is selected, which allows OpenVINO to automatically choose the most suitable device based on the available hardware and the model's requirements. However, you can override this behavior by explicitly selecting a specific device from the list.\n",
    "\n",
    "Once you have selected the inference device, the subsequent steps in the pipeline will be executed on that device, taking full advantage of the OpenVINO runtime's optimizations and acceleration capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cMZZc6X_ULhD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YFwJYwqwXkb4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f307ded3ad4dbd9e6b0c74ed75b9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aa: doesnt work - complaining about moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /opt/app-root/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.25.0 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (5.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=9.2.0 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (10.2.0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (2.36.1)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in /opt/app-root/lib/python3.9/site-packages (from moviepy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (68.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from proglog<=1.0.0->moviepy) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "125WuSPjXkb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import patch_whisper_for_ov_inference, OpenVINOAudioEncoder, OpenVINOTextDecoder\n",
    "\n",
    "patch_whisper_for_ov_inference(model)\n",
    "\n",
    "model.encoder = OpenVINOAudioEncoder(core, WHISPER_ENCODER_OV, device=device.value)\n",
    "model.decoder = OpenVINOTextDecoder(core, WHISPER_DECODER_OV, device=device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwIk3BbwXkb4"
   },
   "source": [
    "## Running the Video Transcription Pipeline\n",
    "\n",
    "With the Whisper model converted to OpenVINO IR format and the inference device selected, we are now ready to run the video transcription pipeline on our chosen video.\n",
    "\n",
    "For the purpose of this tutorial, we will demonstrate the transcription process using a video from YouTube. In the code cell below, you can enter the URL of the YouTube video you wish to transcribe. Please keep in mind that downloading the video may take some time, depending on the video's length and your internet connection speed.\n",
    "\n",
    "Once the video URL is provided, the code will automatically download the video and save it to the local file system. The downloaded video file will serve as the input for the transcription pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1rSEhiKLXkb5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c69980ea7f64ba1a8442e2592a67bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='https://youtu.be/kgL5LBM-hFI', description='Video:', placeholder='Type link for video')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "VIDEO_LINK = \"https://youtu.be/kgL5LBM-hFI\"\n",
    "#VIDEO_LINK = \"https://youtu.be/5bs9XoTac88\"\n",
    "link = widgets.Text(\n",
    "    value=VIDEO_LINK,\n",
    "    placeholder=\"Type link for video\",\n",
    "    description=\"Video:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "link"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "u42YLCJJXkb5",
    "tags": []
   },
   "source": [
    "#aa doesn't work \n",
    "from pytube import YouTube\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Downloading video {link.value} started\")\n",
    "\n",
    "output_file = Path(\"downloaded_video.mp4\")\n",
    "yt = YouTube(link.value)\n",
    "yt.streams.get_highest_resolution().download(filename=output_file)\n",
    "print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YZTn_WCuXkb5",
    "tags": []
   },
   "source": [
    "#aa doesn't work \n",
    "from utils import get_audio\n",
    "\n",
    "audio, duration = get_audio(output_file)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "widgets.Video.from_file(output_file, loop=False, width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhPc92RNXkb5"
   },
   "source": [
    "Select the task for the model:\n",
    "\n",
    "* **transcribe** - generate audio transcription in the source language (automatically detected).\n",
    "* **translate** - generate audio transcription with translation to English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiny.en',\n",
       " 'tiny',\n",
       " 'base.en',\n",
       " 'base',\n",
       " 'small.en',\n",
       " 'small',\n",
       " 'medium.en',\n",
       " 'medium',\n",
       " 'large-v1',\n",
       " 'large-v2',\n",
       " 'large-v3',\n",
       " 'large']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whisper import _MODELS\n",
    "list(_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AY7wVs9lqf01",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475020134a6d46789c6b144dcd2ba04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=3, options=('tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'm…"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whisper import _MODELS\n",
    "import ipywidgets as widgets\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=list(_MODELS),\n",
    "    value='base',\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hxCtWXI4Xkb5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9927e8e8f4e420a8bbd6c7aca8c957c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Select task:', index=1, options=('transcribe', 'translate'), value='translate')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = widgets.Select(\n",
    "    options=[\"transcribe\", \"translate\"],\n",
    "    value=\"translate\",\n",
    "    description=\"Select task:\",\n",
    "    disabled=False\n",
    ")\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transcribe'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ei3b5QupqNAU",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz -o ffmpeg.tar.xz \\\n",
    "     && tar -xf ffmpeg.tar.xz && rm ffmpeg.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/app-root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/opt/app-root/src/.local/bin/:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/mssql-tools18/bin:./ffmpeg-7.0.2-amd64-static\n",
      "/opt/app-root/src/my-whisper/Chapter06/ffmpeg-7.0.2-amd64-static/ffmpeg\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ffmdir = !find . -iname ffmpeg-*-static\n",
    "path = %env PATH\n",
    "path = path + ':' + ffmdir[0]\n",
    "%env PATH $path\n",
    "!which ffmpeg\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio = \"example_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "qC5EHH8MXkb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcription = model.transcribe(audio, fp16=torch.cuda.is_available(), task=task.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6O3UvtmXkb5"
   },
   "source": [
    "\"The results will be saved in the `downloaded_video.srt` file. SRT is one of the most popular formats for storing subtitles and is compatible with many modern video players. This file can be used to embed transcription into videos during playback or by injecting them directly into video files using `ffmpeg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6-wtpXA6Xkb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import prepare_srt\n",
    "\n",
    "#srt_lines = prepare_srt(transcription, filter_duration=duration)\n",
    "srt_lines = prepare_srt(transcription)\n",
    "# save transcription\n",
    "with output_file.with_suffix(\".srt\").open(\"w\") as f:\n",
    "    f.writelines(srt_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sh2y9o1Xkb6"
   },
   "source": [
    "Now let us see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "gJ1ceKJSXkb6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:06,360\n",
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\n",
      "\n",
      "2\n",
      "00:00:06,360 --> 00:00:11,280\n",
      " Nor is Mr. Quilter's manner less interesting than his matter.\n",
      "\n",
      "3\n",
      "00:00:11,280 --> 00:00:16,840\n",
      " He tells us that at this festive season of the year, with Christmas and roast beef looming\n",
      "\n",
      "4\n",
      "00:00:16,840 --> 00:00:23,800\n",
      " before us, similarly he's drawn from eating and its results occur most readily to the mind.\n",
      "\n",
      "5\n",
      "00:00:23,800 --> 00:00:29,400\n",
      " He has graved doubts whether Sir Frederick Layton's work is really Greek after all, and\n",
      "\n",
      "6\n",
      "00:00:29,400 --> 00:00:33,600\n",
      " can discover in it but little of Rocky Ithaca.\n",
      "\n",
      "7\n",
      "00:00:33,600 --> 00:00:39,800\n",
      " Lynelle's pictures are a sort of upgards and atom paintings, and Mason's exquisite\n",
      "\n",
      "8\n",
      "00:00:39,800 --> 00:00:44,600\n",
      " ittles are as national as a jingo poem.\n",
      "\n",
      "9\n",
      "00:00:44,600 --> 00:00:50,360\n",
      " Mr. Birk at Foster's landscapes smile at one much in the same way that Mr. Carker used\n",
      "\n",
      "10\n",
      "00:00:50,360 --> 00:00:52,920\n",
      " to flash his teeth.\n",
      "\n",
      "11\n",
      "00:00:52,920 --> 00:00:58,680\n",
      " And Mr. John Collier gives his sitter a cheerful slap on the back, before he says like\n",
      "\n",
      "12\n",
      "00:00:58,680 --> 00:01:00,760\n",
      " a shampooer in a Turkish bath.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(srt_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnZatuyMXkb6"
   },
   "source": [
    "## Interactive Demo\n",
    "\n",
    "...\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/204548693-1304ef33-c790-490d-8a8b-d5766acb6254.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Speech Recognition"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
