{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5a9f184-d674-4d1b-a284-f82b3629eb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grettings to : https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb511-8b73-4053-a194-96a737191b55",
   "metadata": {
    "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the tutorial, ensure that you have the necessary prerequisites in place. This includes authenticating with the Hugging Face Hub using your token and verifying the authentication by running the provided code cells. These steps are crucial for accessing the required models and datasets throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b9d482-02fc-46b5-852d-22f3f7178f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/app-root/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b370797-5328-4ba6-8ff4-50289e3648d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17071a18-2bea-4a0e-afcb-5aff8bb09a15",
   "metadata": {
    "id": "CFK6tOV0bLnE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4a5ef31f804b90a6dfe1096d767ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367e55be-f67a-47c7-9333-8be91bbf91ae",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '676187706ce06148154eb2f8',\n",
       " 'name': 'aaiche',\n",
       " 'fullname': 'aaiche',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/1bf3857a4d2a4e5e6fb5381ba1ed7752.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'AA_TOKEN',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-01-11T15:38:55.885Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '676187706ce06148154eb2f8',\n",
       "       'type': 'user',\n",
       "       'name': 'aaiche'},\n",
       "      'permissions': ['repo.content.read']}]}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify authentication\n",
    "from huggingface_hub import whoami\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15866060-1d89-44b5-b40a-77a60484fa3b",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd604-9abe-4fd1-bb9c-76d099a39cba",
   "metadata": {
    "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
   },
   "source": [
    "## Loading the PyTorch model\n",
    "\n",
    "Loading the PyTorch Whisper model is a straightforward process using the transformers library. The `AutoModelForSpeechSeq2Seq.from_pretrained` method is employed to initialize the model. In this tutorial, we will use the `distil-whisper/distil-large-v2` model as the default example. Please note that the model will be downloaded during the first run, which may take some time.\n",
    "\n",
    "However, you have the flexibility to choose from a variety of models available in the [Distil-Whisper Hugging Face collection](https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6). Some alternative options include `distil-whisper/distil-medium.en` and `distil-whisper/distil-small.en`. Additionally, models of the original Whisper architecture are also accessible, which you can explore further [here](https://huggingface.co/openai).\n",
    "\n",
    "It's important to highlight the significance of preprocessing and post-processing in the model's usage. The `AutoProcessor` class, specifically the `WhisperProcessor`, plays a crucial role in preparing the audio input data for the model. It handles tasks such as converting the audio to a Mel-spectrogram representation and decoding the predicted output token IDs back into a string using the tokenizer.\n",
    "\n",
    "To ensure a smooth and efficient workflow, the `AutoProcessor` class streamlines the preprocessing and post-processing steps, allowing you to focus on the core functionality of the Whisper model. By leveraging this class, you can easily integrate the Whisper model into your speech recognition pipeline, regardless of the specific model variant you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9518b369-cf8d-426d-b034-c89e14f6e39a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fee4dab4af144c69977cd71bd00c594c",
      "5f252a0c901040bc8f4850f3f2013109",
      "f872a9052ace4694b411dc6981d4a1df"
     ]
    },
    "id": "uqh7J71HNsYG",
    "outputId": "16413215-4b75-4601-c0db-26c2005796eb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bea9b033f34307a5702c3b99d02690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model type:', index=1, options=('Distil-Whisper', 'Whisper'), value='Whisper')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = {\n",
    "    \"Distil-Whisper\": [\n",
    "        \"distil-whisper/distil-large-v2\",\n",
    "        \"distil-whisper/distil-medium.en\",\n",
    "        \"distil-whisper/distil-small.en\"\n",
    "    ],\n",
    "    \"Whisper\": [\n",
    "        \"openai/whisper-large-v3\",\n",
    "        \"openai/whisper-large-v2\",\n",
    "        \"openai/whisper-large\",\n",
    "        \"openai/whisper-medium\",\n",
    "        \"openai/whisper-small\",\n",
    "        \"openai/whisper-base\",\n",
    "        \"openai/whisper-tiny\",\n",
    "        \"openai/whisper-medium.en\",\n",
    "        \"openai/whisper-small.en\",\n",
    "        \"openai/whisper-base.en\",\n",
    "        \"openai/whisper-tiny.en\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_type = widgets.Dropdown(\n",
    "    options=model_ids.keys(),\n",
    "    value=\"Whisper\",\n",
    "    description=\"Model type:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f47a3b8-03f3-4dde-a5dd-267091952e5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18906804c7dc46cfa8e3b1428349d4e8",
      "522feef58e8e4804b610649249bddcf8",
      "fc63388bc1bf45f2bc817f2e408ab507"
     ]
    },
    "id": "WHkwWcsrNsYG",
    "outputId": "e7203767-d08d-4091-cee1-ecf282d950fe",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdba2d9cbd940e0a7ea7649598b5497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('openai/whisper-large-v3', 'openai/whisper-large-v2', 'openai…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids[model_type.value],\n",
    "    value=model_ids[model_type.value][1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db265ac1-dc18-48cd-8d9b-772484c3fee0",
   "metadata": {
    "id": "zilL_lNDSRJ4"
   },
   "source": [
    "## Step 1: Loading the Transformers ASR Model\n",
    "\n",
    "To begin building our speech recognition demo, we first need to have an Automatic Speech Recognition (ASR) model. You can either train your own model or use a pre-trained one. In this tutorial, we will leverage a pre-trained ASR model called \"whisper\" from OpenAI.\n",
    "\n",
    "Loading the \"whisper\" model from the Hugging Face Transformers library is a straightforward process. Here's the code snippet to accomplish this:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "p = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "```\n",
    "\n",
    "With just these two lines of code, we initialize a pipeline for automatic speech recognition using the \"openai/whisper-base.en\" model. The pipeline abstracts away the complexities of working with the model directly, providing a high-level interface for performing ASR tasks.\n",
    "\n",
    "By utilizing a pre-trained model like \"whisper\", we can quickly get started with building our demo without the need for extensive model training. This allows us to focus on integrating the model into our application and creating an engaging user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176271-c4cb-46e7-9ae9-ed5a1bd33588",
   "metadata": {
    "id": "Q6F2DkegPQ1h"
   },
   "source": [
    "## Step 2: Building a Full-Context ASR Demo with Transformers\n",
    "\n",
    "Our first step in creating the speech recognition demo is to build a *full-context* ASR demo. In this demo, the user will speak the entire audio before the ASR model processes it and generates the transcription. Thanks to Gradio's intuitive interface, building this demo is a breeze.\n",
    "\n",
    "We'll start by creating a function that wraps around the `pipeline` object we initialized earlier. This function will serve as the core of our demo, handling the audio input and generating the transcription.\n",
    "\n",
    "To capture the user's audio input, we'll utilize Gradio's built-in `Audio` component. This component will be configured to accept input from the user's microphone and return the filepath of the recorded audio. For displaying the transcribed text, we'll use a simple `Textbox` component.\n",
    "\n",
    "The `transcribe` function, which is the heart of our demo, takes a single parameter called `audio`. This parameter represents the audio data recorded by the user, stored as a NumPy array. However, the `pipeline` object expects the audio data to be in the `float32` format. To ensure compatibility, we first convert the audio data to `float32` and then normalize it by dividing it by its maximum absolute value. Finally, we pass the processed audio data to the `pipeline` object to obtain the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df639a82-db81-43a3-b9c5-72766afce976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x. 5 1000740000 root 128 Jan 13 10:20 .\n",
      "drwxr-xr-x. 4 1000740000 root  67 Jan 13 10:19 ..\n",
      "drwxr-xr-x. 6 1000740000 root  65 Jan 13 10:18 datasets--hf-internal-testing--librispeech_asr_dummy\n",
      "drwxr-xr-x. 4 1000740000 root 114 Jan 13 10:20 .locks\n",
      "drwxr-xr-x. 6 1000740000 root  65 Jan 13 10:20 models--distil-whisper--distil-medium.en\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a369721-4b1b-4393-a879-50d33e533f20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "3d336c21e2c94e11999300d5be080c93",
      "017752c1241b42adb5588c9a17c6f720",
      "9723e5270fa54cb0aaacaa5727e78b82",
      "9b8923adfea54139bbe4704dab6700e0",
      "f9abec2dc1e1461d8737938d1b656eff",
      "9b2048a14c5f4a15a240de505db0ef71",
      "66b7c77d1ea7449dbc536654ae3a19e9",
      "46bdd548eb364f0dbc8404474ccd5c6e",
      "044d24485c6f40debcce50f46d999806",
      "df30d496022a43c684d0098deefdc7d8",
      "e1397cce85524490bafcce0949535c9e",
      "6709089d0b264cc7a53e592a8849ae4e",
      "413749954b7b4ca497b7d2152248ddbc",
      "da6535e37d9b454db1d1fd9c8143b426",
      "fc06df4071ab4d21899564b393270527",
      "15f2a143de9e4b1bb5b411eec29ef42a",
      "ffc68ab79c824f149dee353b6e84d06a",
      "17923d2aeaa84228beda6114210ed726",
      "9a37299d0e72431284e7c6d15b7645c2",
      "8a5219c49563466db82ea06f31d7a862",
      "a882380256e647fb8f881f4c2857b29a",
      "8a6810a87cf54deba34e6bc17d76bb77",
      "f53be024380c42b6949cc9ce29b69541",
      "b4183b280d8d4276850267beba60d6eb",
      "5761d7aa32e34665b0f05a1d57c50164",
      "b629cf2d8ced4d0c966ef50f95a31f50",
      "108c5e113f984cfaa6c84ca0b5672944",
      "8c61efd854a34bebba33725a49996716",
      "f4f9647247ac421186cbefe79a2ceb8c",
      "fce8c66829354cddbeaa5d56de2594e2",
      "58cb82243a9440558fe6acd0b9e3c043",
      "046716c66572443e8224874b54eed3b3",
      "4254e413534e4b1cada2779f5be838e6",
      "feb0b610d1eb4440835239b10dc02a59",
      "4e590a2b27b74343bf6b6004a7c2ed2d",
      "091d22a6af054115a33ccb7b1fde2417",
      "3f9addd6893c42e9b288ef8738dbb38c",
      "70f0e385ff084155a156089b39100c10",
      "07f9e3adc1c74275943cd6d9b22266b6",
      "ee877da8550945db81953805f778f4c4",
      "819c41022bec4d779e6665c2d28900ef",
      "0460068f54224027ad89207af574ab30",
      "fa55d5e1288b4caab46c025da64d9ecf",
      "c02b061234cb4659bc7f784a41a111db",
      "30e0ddc8c03b4ad8bc33f356c66c964b",
      "30fb46e0ea0d485ea6e5540306b74412",
      "0c4cef46a2074bbabb4254abe14118a6",
      "48c080cd107a49068e38ce9c9ff7152b",
      "abd6cc02f9134fb989492ebdbc2a65b1",
      "1aa7a2221b9440a0a5b23ce66b83f4ef",
      "e6d0fe0ca1af453b8e685d4026cf7b2e",
      "17f02827fc1a4d8aa26dfffc759f504b",
      "ab88a20e95f84ffbb188ebe56f8677fd",
      "e36d5f8d051e42e28a06e4e48d28ac39",
      "e688b7cca32a44f39dc08a360c538fa6",
      "d6b82a8c30164cd2b3b82ef58a0e8b49",
      "e7eab28a118146c08d1f5a14286b9f00",
      "ebc58f9829d147718e8f7362dc491a39",
      "b4c19b0475e541cd98ec68bd91939696",
      "261ff4d16895488095543db10f3abe4f",
      "1d8d4c53e6c6435da57f2b57e7503396",
      "292176279ece40019e3104d5779d3fbf",
      "426d4cdafb484ca9bbd189c3b9995c92",
      "3e321f2163a341c7acf349b4d7feb6c2",
      "d6785dd1ab3745f380cf8e1a846434fd",
      "50401796c2334720abf382399db3d280",
      "5637690ab3b646e3a9bd63683bedce40",
      "1eab1176f2ea420988bb9ba9e3b9263e",
      "369b51e6c05c4b8690d08114038fff42",
      "513b8d0fb9b94f088d010d0e358ac80b",
      "2ebcbbdd506a466aa7ff65eb52c8a8e1",
      "467fea05a7054eab8ec6caec0f093290",
      "ccab9b9588ed4d8bb8f47f5b4ddbed3e",
      "64be9f5414aa4510813e3ff7bd912261",
      "19ba69de75ae4df283737cb75534563d",
      "934997d106344543acf286795bc30c6b",
      "81a7a3bb1117401d947068d0d2a79332",
      "534241a659b942d09f664816b8db06ab",
      "692897f7a6db42f6a206f470b474a353",
      "9eb90aacb8fa4a689bc6cd3e3dc606e3",
      "55ce55d4429c416ba0ea8258f71dcaa2",
      "008eabbba79344f9887eae5a274c01cd",
      "bc4248579eb14f87b1b85a07feb3e02a",
      "ac8ccc097ca849dcb9d1d9bc1cecc703",
      "14a3438ecab94c95b3cfd8cc175b1677",
      "a2ec00e871d74b8db61f340e84e49e25",
      "d3764b93d8f74989867c52852d1d6f4d",
      "f3424efa869d4b579e2cba89e69e4976",
      "114a2b3336e948899bcb950d55ec4142",
      "2f47881ed5684234a5c97e0af7886c1d",
      "e277cfd7978247e59d0904dfd086c701",
      "a2af0b49f54c40a598fe53f0b7144c72",
      "c14ad6fad4324cd29cc6178f63a8cded",
      "f995edda20164c0b9d90a0b8059c99c4",
      "1111d705b090448cbace9c2c29ea93fa",
      "20ec443d74434922b645266130249a3a",
      "0dbd380aec4b4e7ab481d29066616723",
      "f734996feeea41f38413c8d50b8d7c40",
      "338dc261d2944b32aaff162e8881d269",
      "0ab2d3e9aee44fc2be4ee368ce38f9ac",
      "d7463c288c6742a59cb09541189bd068",
      "d327266c38624373b0483d704e8be178",
      "3a9fa0f70408469eb10938e1497e3b74",
      "6f49cc1902ab4d64b6363b274b324806",
      "e4557e8f3acb457da608871744be899c",
      "bf060d173342453fbad4d1af4949be18",
      "96633ab9838e460b8528eeef2854f46c",
      "b7d5792bc8014f2c9d664d2ab08fda3c",
      "cfddc859a8764139a8adc9d80227d4d3",
      "24fb51e2e9654c6e98f628a096b1aad0",
      "4b1bee5600b1499ca5d3ca4e190137fd",
      "572dc3481b1348578178bc7e52436374",
      "da3729683bf047b9a3ce0df58b9e6e2d",
      "c1ad03eb552145d086dca5650f7256e6",
      "28748337e6db437f81a8bc3cc7ccc218",
      "6954ca08b16343638408cf74a2c65d42",
      "9c0dafbf3b05446caa8774e7bb2d540e",
      "5751e9490ed94c92ac776a48656394e1",
      "4dd8ad050173478f9c55c506f786da9a",
      "90a05d16f45a4ea687373ec924d56c93",
      "2e62859f509948c494871e4f6e2177a6"
     ]
    },
    "id": "e5382431-497e-4688-b4ec-8958a92163e7",
    "outputId": "97407080-6b57-4cc3-e68c-459512f01a00",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c303e2b17144b994dba1df70643f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403217ff0f244e009d43a30348cf8a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616b0a4d87e4d199e144ee117eff656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9913e48fcf7423bb08e6f1ec21409aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518d087b129a4168be077f7f3b33ac66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff31c310c4b43fa9151c7c372368036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b0559ed0174ddd8f4b6b6a5ca83fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d20fb2d77a34a0b808734063dda30f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc29ec0fd284557a6433363715c4991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05ebec60a134fd48be9bbc5474b1a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cebedf74df437bbeecc2e20694a63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536f8a2937e54c14b755140fde74b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id.value)\n",
    "\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
    "pt_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248d548d-de49-4559-893b-6c29890b510c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x. 6 1000740000 root 187 Jan 13 10:35 .\n",
      "drwxr-xr-x. 4 1000740000 root  67 Jan 13 10:19 ..\n",
      "drwxr-xr-x. 6 1000740000 root  65 Jan 13 10:18 datasets--hf-internal-testing--librispeech_asr_dummy\n",
      "drwxr-xr-x. 5 1000740000 root 154 Jan 13 10:35 .locks\n",
      "drwxr-xr-x. 6 1000740000 root  65 Jan 13 10:20 models--distil-whisper--distil-medium.en\n",
      "drwxr-xr-x. 6 1000740000 root  65 Jan 13 10:37 models--openai--whisper-large-v2\n",
      "-rw-r--r--. 1 1000740000 root   1 Jan 13 10:35 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f957d-1f8c-4629-a21a-e0abaf34e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2033d404-00d8-41a1-bd89-2eb47aa7ea80",
   "metadata": {
    "id": "IRS8EXpwFZlR",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "this_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "this_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a6d7c8-71df-4307-ae53-98ad6569f6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1674d724-51d0-430c-9c50-0102f2de9f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aa: comment - pytorch doesnt have the tool"
   ]
  },
  {
   "cell_type": "raw",
   "id": "337c4798-f2f1-4532-b9aa-3c3cd75dc904",
   "metadata": {
    "tags": []
   },
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78476-5d6b-40d2-9bb4-4df5768f400c",
   "metadata": {
    "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
   },
   "source": [
    "### Preparing the Input Sample\n",
    "\n",
    "To use the Whisper model for speech recognition, we need to properly prepare the input audio sample. The `WhisperProcessor` expects the audio data to be in the form of a NumPy array, along with information about the audio sampling rate. It then processes the audio and returns the `input_features` tensor, which is used for making predictions.\n",
    "\n",
    "The conversion of the audio file to the required NumPy format is conveniently handled by the Hugging Face Datasets library. This library provides a seamless interface for loading and preprocessing audio data, making it easier to integrate with the Whisper model.\n",
    "\n",
    "To prepare the input sample, the next Python code:\n",
    "\n",
    "1. Loads the audio file using the Hugging Face Datasets library.\n",
    "2. Extracts the audio data as a NumPy array and obtain the sampling rate.\n",
    "3. Passes the audio array and sampling rate to the `WhisperProcessor`.\n",
    "4. Retrieves the `input_features` tensor from the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a63e8c-dee7-4ac7-a32c-4fe74a335088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x. 4 1000740000 root 4.0K Jan 13 10:38 .\n",
      "drwxr-xr-x. 4 1000740000 root   67 Jan 13 10:19 ..\n",
      "drwxr-xr-x. 3 1000740000 root   19 Jan 13 10:38 distil-whisper___librispeech_long\n",
      "drwxr-xr-x. 3 1000740000 root   19 Jan 13 10:13 hf-internal-testing___librispeech_asr_dummy\n",
      "-rw-r--r--. 1 1000740000 root    0 Jan 13 10:38 _opt_app-root_src_.cache_huggingface_datasets_distil-whisper___librispeech_long_clean_0.0.0_164d3b41852b1eebe89f1dc0e6e0042f16835ea0.lock\n",
      "-rw-r--r--. 1 1000740000 root    0 Jan 13 10:18 _opt_app-root_src_.cache_huggingface_datasets_hf-internal-testing___librispeech_asr_dummy_clean_0.0.0_5be91486e11a2d616f4ec5db8d3fd248585ac07a.lock\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b93df-a496-4847-bb19-c400d540d69b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "54f80c042875442cbc4886b33a7fed1b",
      "a763b3e0cf794b81b0e1d58dd6cd2fdc",
      "b8e96e211f96493f9bafdc9a8f8083fb",
      "3b29eaf965b74f8b8a928f7e1de00a71",
      "85126b4bc6c241b0b6ac6f0fc16d2fc4",
      "ddc61dc02f8c44da9dd1ce8f3828e379",
      "ef45cbd9e7744aae938b654ecb5eac11",
      "287c0aa4ae014130afdf66c79026f284",
      "0c17481318eb4f65a2e4afef66446916",
      "1c2c78b8560d4e84a3d8a63eb90e6a9f",
      "4e6c41945a704a579e6a455bb598cc56",
      "610b9c4c9e12491cb5833d3bc3dd300a",
      "063fe5fe1af74e6489b9f265b3541e09",
      "cdc1772e03d54b2c9d48a9e668156c4d",
      "4e00c89d62044e27aa5a37d8561d9a4d",
      "207b8969abf14a028dfd650b4378dd14",
      "0efcadf24d04404fb85c4d7a8556df4b",
      "0c5dc31513864bf995bda491fb83b0cf",
      "0b5ab99e70924d4681c35cca99eb26f6",
      "7ad382ac2fee4ca3bdd53a743dd7ca9b",
      "316fc74123eb498f9b867f35c6bde185",
      "5e426aa6112940db8de9017616f159dc",
      "473f60c52bf847a8bb2279b9633f913a",
      "47fa45bebf4c4659a77b258d980e37aa",
      "2dbca1333acc4e17af43239de903bbfd",
      "8edef4788f284f9d9d240dd927e391b4",
      "346e9644dd5b476db03fb322758142e9",
      "29c2e2ec675a485d89e70d1accb93eb1",
      "95e98ef53d4a4da194a3afd7e5a4e3d4",
      "ffe1e64502f44624b8462bb8b0882e51",
      "ea75c05a7f3749ff9cc0980470d4a70b",
      "9f6413789c284a2cafb73a2b46d2e261",
      "c167041d90184f0fb47646759bda9148"
     ]
    },
    "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111",
    "outputId": "03c77fb1-b50f-416c-949b-35cb3e61b2ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_input_features(sample):\n",
    "    input_features = processor(\n",
    "        sample[\"audio\"][\"array\"],\n",
    "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62dab4-33b4-4d66-99af-f06b0b616f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -alh  ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c25625-7462-4a3b-8ca8-77c65dd0dcc9",
   "metadata": {
    "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
   },
   "source": [
    "### Running Model Inference\n",
    "\n",
    "With the input sample prepared, we can now perform speech recognition using the Whisper model. The model provides a convenient `generate` interface that simplifies the inference process. Here's how you can run the model inference:\n",
    "\n",
    "1. Pass the `input_features` tensor to the `generate` method of the Whisper model.\n",
    "2. The model will process the input and generate the predicted token IDs.\n",
    "3. Once the generation is complete, use the `processor.batch_decode` method to decode the predicted token IDs into human-readable text transcription.\n",
    "\n",
    "The `generate` method handles the complex task of sequence generation, taking into account the model's architecture and the provided input features. It produces the predicted token IDs, which represent the transcribed text in a encoded format.\n",
    "\n",
    "By leveraging the `generate` interface and the `processor.batch_decode` method, you can easily perform speech recognition with the Whisper model. The model takes care of the complex task of mapping the audio input to text output, while the processor handles the necessary decoding step to provide you with the final transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62840b-69a7-4dff-bca0-2226a28a87a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "c9618867-beae-4875-a5be-0e0a3b453414",
    "outputId": "7733e9a6-6f7a-417d-b4ee-1e99f2aec090",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import IPython.display as ipd\n",
    "\n",
    "predicted_ids = pt_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "print(f\"Reference: {sample['text']}\")\n",
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb97e-3c2b-4a48-a648-1488fd30d127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658623-0581-4daa-9bae-1a61e4bb8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db79c40-0d3e-4eee-a095-8df9ff17a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
