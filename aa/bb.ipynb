{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d9bd72-189b-42c0-8f94-eb698ee3055e",
   "metadata": {},
   "source": [
    "Grettings to :\n",
    "https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb511-8b73-4053-a194-96a737191b55",
   "metadata": {
    "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the tutorial, ensure that you have the necessary prerequisites in place. This includes authenticating with the Hugging Face Hub using your token and verifying the authentication by running the provided code cells. These steps are crucial for accessing the required models and datasets throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b9d482-02fc-46b5-852d-22f3f7178f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.27.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31185b55-3933-4760-9d35-a8c551aa1726",
   "metadata": {
    "id": "VCEKs-Y4wAYQ"
   },
   "source": [
    "## Setting up a Hugging Face account\n",
    "\n",
    "To access and utilize the vast array of datasets and machine learning models available on Hugging Face, an account is required. A Hugging Face account offers not just access to datasets like \"PolyAI/minds14\" but also allows for collaboration on projects, contribution to the community through dataset/model sharing, and even tracking progress on various machine learning tasks. Follow the steps outlined above to create your account and get started with Hugging Face.\n",
    "\n",
    "## To create a Hugging Face account, you can follow these simple steps:\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_join.png\" width=250>\n",
    "\n",
    "1. Go to the Hugging Face website.\n",
    "2. Click on “Sign Up” or go to https://huggingface.co/join\n",
    "3. Enter your email address and password (you can skip the prompt to \"Join organization\").\n",
    "4. If you already have an account, click on “Log in”.\n",
    "5. To interact with the Hub, you need to be logged in with a Hugging Face account.\n",
    "\n",
    "## To create a Hugging Face Access Token\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_settings_menu_option.png\" width=650>\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_create_token.png\" width=250>\n",
    "\n",
    "1. Go to the Access Tokens tab in your Hugging Face settings.\n",
    "2. Click on the New token button to create a new User Access Token.\n",
    "3. Select the `write` role and a name for your token. This notebook only needs a `read` role; however, we will be writing to Hugging Face in [Chapter 4](https://colab.research.google.com/drive/1LADNomT0JUBCsopU6r_NsZfOaNiz2N3h).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17071a18-2bea-4a0e-afcb-5aff8bb09a15",
   "metadata": {
    "id": "CFK6tOV0bLnE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beceefaaa8343ac932352189074ae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367e55be-f67a-47c7-9333-8be91bbf91ae",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '676187706ce06148154eb2f8',\n",
       " 'name': 'aaiche',\n",
       " 'fullname': 'aaiche',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/1bf3857a4d2a4e5e6fb5381ba1ed7752.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'AA_TOKEN',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-01-11T15:38:55.885Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '676187706ce06148154eb2f8',\n",
       "       'type': 'user',\n",
       "       'name': 'aaiche'},\n",
       "      'permissions': ['repo.content.read']}]}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify authentication\n",
    "from huggingface_hub import whoami\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15866060-1d89-44b5-b40a-77a60484fa3b",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-24.3.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets[audio]\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (1.5.3)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets[audio])\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets[audio])\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets[audio]) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (3.9.3)\n",
      "Collecting soundfile>=0.12.1 (from datasets[audio])\n",
      "  Downloading soundfile-0.13.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting librosa (from datasets[audio])\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting soxr>=0.4.0 (from datasets[audio])\n",
      "  Downloading soxr-0.5.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/app-root/lib/python3.9/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Collecting audioread>=2.1.9 (from librosa->datasets[audio])\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa->datasets[audio])\n",
      "  Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pooch>=1.1 (from librosa->datasets[audio])\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting lazy-loader>=0.1 (from librosa->datasets[audio])\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets[audio]) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets[audio]) (2024.1)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa->datasets[audio])\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/app-root/lib/python3.9/site-packages (from pooch>=1.1->librosa->datasets[audio]) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.3.0)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m343.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading soundfile-0.13.0-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m167.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m159.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, soxr, safetensors, requests, regex, multiprocess, llvmlite, lazy-loader, audioread, soundfile, pooch, numba, tokenizers, librosa, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed audioread-3.0.1 datasets-3.2.0 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 multiprocess-0.70.16 numba-0.60.0 pooch-1.8.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 soundfile-0.13.0 soxr-0.5.0.post1 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd604-9abe-4fd1-bb9c-76d099a39cba",
   "metadata": {
    "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
   },
   "source": [
    "## Loading the PyTorch model\n",
    "\n",
    "Loading the PyTorch Whisper model is a straightforward process using the transformers library. The `AutoModelForSpeechSeq2Seq.from_pretrained` method is employed to initialize the model. In this tutorial, we will use the `distil-whisper/distil-large-v2` model as the default example. Please note that the model will be downloaded during the first run, which may take some time.\n",
    "\n",
    "However, you have the flexibility to choose from a variety of models available in the [Distil-Whisper Hugging Face collection](https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6). Some alternative options include `distil-whisper/distil-medium.en` and `distil-whisper/distil-small.en`. Additionally, models of the original Whisper architecture are also accessible, which you can explore further [here](https://huggingface.co/openai).\n",
    "\n",
    "It's important to highlight the significance of preprocessing and post-processing in the model's usage. The `AutoProcessor` class, specifically the `WhisperProcessor`, plays a crucial role in preparing the audio input data for the model. It handles tasks such as converting the audio to a Mel-spectrogram representation and decoding the predicted output token IDs back into a string using the tokenizer.\n",
    "\n",
    "To ensure a smooth and efficient workflow, the `AutoProcessor` class streamlines the preprocessing and post-processing steps, allowing you to focus on the core functionality of the Whisper model. By leveraging this class, you can easily integrate the Whisper model into your speech recognition pipeline, regardless of the specific model variant you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9518b369-cf8d-426d-b034-c89e14f6e39a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fee4dab4af144c69977cd71bd00c594c",
      "5f252a0c901040bc8f4850f3f2013109",
      "f872a9052ace4694b411dc6981d4a1df"
     ]
    },
    "id": "uqh7J71HNsYG",
    "outputId": "16413215-4b75-4601-c0db-26c2005796eb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393dc73b53834e32bb790c2d532e4b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model type:', index=1, options=('Distil-Whisper', 'Whisper'), value='Whisper')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = {\n",
    "    \"Distil-Whisper\": [\n",
    "        \"distil-whisper/distil-large-v2\",\n",
    "        \"distil-whisper/distil-medium.en\",\n",
    "        \"distil-whisper/distil-small.en\"\n",
    "    ],\n",
    "    \"Whisper\": [\n",
    "        \"openai/whisper-large-v3\",\n",
    "        \"openai/whisper-large-v2\",\n",
    "        \"openai/whisper-large\",\n",
    "        \"openai/whisper-medium\",\n",
    "        \"openai/whisper-small\",\n",
    "        \"openai/whisper-base\",\n",
    "        \"openai/whisper-tiny\",\n",
    "        \"openai/whisper-medium.en\",\n",
    "        \"openai/whisper-small.en\",\n",
    "        \"openai/whisper-base.en\",\n",
    "        \"openai/whisper-tiny.en\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_type = widgets.Dropdown(\n",
    "    options=model_ids.keys(),\n",
    "    value=\"Whisper\",\n",
    "    description=\"Model type:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f47a3b8-03f3-4dde-a5dd-267091952e5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18906804c7dc46cfa8e3b1428349d4e8",
      "522feef58e8e4804b610649249bddcf8",
      "fc63388bc1bf45f2bc817f2e408ab507"
     ]
    },
    "id": "WHkwWcsrNsYG",
    "outputId": "e7203767-d08d-4091-cee1-ecf282d950fe",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbdf01906ea4e37b1b34a94ee3620bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('openai/whisper-large-v3', 'openai/whisper-large-v2', 'openai…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids[model_type.value],\n",
    "    value=model_ids[model_type.value][1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db265ac1-dc18-48cd-8d9b-772484c3fee0",
   "metadata": {
    "id": "zilL_lNDSRJ4"
   },
   "source": [
    "## Step 1: Loading the Transformers ASR Model\n",
    "\n",
    "To begin building our speech recognition demo, we first need to have an Automatic Speech Recognition (ASR) model. You can either train your own model or use a pre-trained one. In this tutorial, we will leverage a pre-trained ASR model called \"whisper\" from OpenAI.\n",
    "\n",
    "Loading the \"whisper\" model from the Hugging Face Transformers library is a straightforward process. Here's the code snippet to accomplish this:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "p = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "```\n",
    "\n",
    "With just these two lines of code, we initialize a pipeline for automatic speech recognition using the \"openai/whisper-base.en\" model. The pipeline abstracts away the complexities of working with the model directly, providing a high-level interface for performing ASR tasks.\n",
    "\n",
    "By utilizing a pre-trained model like \"whisper\", we can quickly get started with building our demo without the need for extensive model training. This allows us to focus on integrating the model into our application and creating an engaging user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176271-c4cb-46e7-9ae9-ed5a1bd33588",
   "metadata": {
    "id": "Q6F2DkegPQ1h"
   },
   "source": [
    "## Step 2: Building a Full-Context ASR Demo with Transformers\n",
    "\n",
    "Our first step in creating the speech recognition demo is to build a *full-context* ASR demo. In this demo, the user will speak the entire audio before the ASR model processes it and generates the transcription. Thanks to Gradio's intuitive interface, building this demo is a breeze.\n",
    "\n",
    "We'll start by creating a function that wraps around the `pipeline` object we initialized earlier. This function will serve as the core of our demo, handling the audio input and generating the transcription.\n",
    "\n",
    "To capture the user's audio input, we'll utilize Gradio's built-in `Audio` component. This component will be configured to accept input from the user's microphone and return the filepath of the recorded audio. For displaying the transcribed text, we'll use a simple `Textbox` component.\n",
    "\n",
    "The `transcribe` function, which is the heart of our demo, takes a single parameter called `audio`. This parameter represents the audio data recorded by the user, stored as a NumPy array. However, the `pipeline` object expects the audio data to be in the `float32` format. To ensure compatibility, we first convert the audio data to `float32` and then normalize it by dividing it by its maximum absolute value. Finally, we pass the processed audio data to the `pipeline` object to obtain the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e9e778-9529-4e77-9c23-d142416acaef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/huggingface/hub': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f32dae2-5b2a-4c41-9558-3f2775b22122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a369721-4b1b-4393-a879-50d33e533f20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "3d336c21e2c94e11999300d5be080c93",
      "017752c1241b42adb5588c9a17c6f720",
      "9723e5270fa54cb0aaacaa5727e78b82",
      "9b8923adfea54139bbe4704dab6700e0",
      "f9abec2dc1e1461d8737938d1b656eff",
      "9b2048a14c5f4a15a240de505db0ef71",
      "66b7c77d1ea7449dbc536654ae3a19e9",
      "46bdd548eb364f0dbc8404474ccd5c6e",
      "044d24485c6f40debcce50f46d999806",
      "df30d496022a43c684d0098deefdc7d8",
      "e1397cce85524490bafcce0949535c9e",
      "6709089d0b264cc7a53e592a8849ae4e",
      "413749954b7b4ca497b7d2152248ddbc",
      "da6535e37d9b454db1d1fd9c8143b426",
      "fc06df4071ab4d21899564b393270527",
      "15f2a143de9e4b1bb5b411eec29ef42a",
      "ffc68ab79c824f149dee353b6e84d06a",
      "17923d2aeaa84228beda6114210ed726",
      "9a37299d0e72431284e7c6d15b7645c2",
      "8a5219c49563466db82ea06f31d7a862",
      "a882380256e647fb8f881f4c2857b29a",
      "8a6810a87cf54deba34e6bc17d76bb77",
      "f53be024380c42b6949cc9ce29b69541",
      "b4183b280d8d4276850267beba60d6eb",
      "5761d7aa32e34665b0f05a1d57c50164",
      "b629cf2d8ced4d0c966ef50f95a31f50",
      "108c5e113f984cfaa6c84ca0b5672944",
      "8c61efd854a34bebba33725a49996716",
      "f4f9647247ac421186cbefe79a2ceb8c",
      "fce8c66829354cddbeaa5d56de2594e2",
      "58cb82243a9440558fe6acd0b9e3c043",
      "046716c66572443e8224874b54eed3b3",
      "4254e413534e4b1cada2779f5be838e6",
      "feb0b610d1eb4440835239b10dc02a59",
      "4e590a2b27b74343bf6b6004a7c2ed2d",
      "091d22a6af054115a33ccb7b1fde2417",
      "3f9addd6893c42e9b288ef8738dbb38c",
      "70f0e385ff084155a156089b39100c10",
      "07f9e3adc1c74275943cd6d9b22266b6",
      "ee877da8550945db81953805f778f4c4",
      "819c41022bec4d779e6665c2d28900ef",
      "0460068f54224027ad89207af574ab30",
      "fa55d5e1288b4caab46c025da64d9ecf",
      "c02b061234cb4659bc7f784a41a111db",
      "30e0ddc8c03b4ad8bc33f356c66c964b",
      "30fb46e0ea0d485ea6e5540306b74412",
      "0c4cef46a2074bbabb4254abe14118a6",
      "48c080cd107a49068e38ce9c9ff7152b",
      "abd6cc02f9134fb989492ebdbc2a65b1",
      "1aa7a2221b9440a0a5b23ce66b83f4ef",
      "e6d0fe0ca1af453b8e685d4026cf7b2e",
      "17f02827fc1a4d8aa26dfffc759f504b",
      "ab88a20e95f84ffbb188ebe56f8677fd",
      "e36d5f8d051e42e28a06e4e48d28ac39",
      "e688b7cca32a44f39dc08a360c538fa6",
      "d6b82a8c30164cd2b3b82ef58a0e8b49",
      "e7eab28a118146c08d1f5a14286b9f00",
      "ebc58f9829d147718e8f7362dc491a39",
      "b4c19b0475e541cd98ec68bd91939696",
      "261ff4d16895488095543db10f3abe4f",
      "1d8d4c53e6c6435da57f2b57e7503396",
      "292176279ece40019e3104d5779d3fbf",
      "426d4cdafb484ca9bbd189c3b9995c92",
      "3e321f2163a341c7acf349b4d7feb6c2",
      "d6785dd1ab3745f380cf8e1a846434fd",
      "50401796c2334720abf382399db3d280",
      "5637690ab3b646e3a9bd63683bedce40",
      "1eab1176f2ea420988bb9ba9e3b9263e",
      "369b51e6c05c4b8690d08114038fff42",
      "513b8d0fb9b94f088d010d0e358ac80b",
      "2ebcbbdd506a466aa7ff65eb52c8a8e1",
      "467fea05a7054eab8ec6caec0f093290",
      "ccab9b9588ed4d8bb8f47f5b4ddbed3e",
      "64be9f5414aa4510813e3ff7bd912261",
      "19ba69de75ae4df283737cb75534563d",
      "934997d106344543acf286795bc30c6b",
      "81a7a3bb1117401d947068d0d2a79332",
      "534241a659b942d09f664816b8db06ab",
      "692897f7a6db42f6a206f470b474a353",
      "9eb90aacb8fa4a689bc6cd3e3dc606e3",
      "55ce55d4429c416ba0ea8258f71dcaa2",
      "008eabbba79344f9887eae5a274c01cd",
      "bc4248579eb14f87b1b85a07feb3e02a",
      "ac8ccc097ca849dcb9d1d9bc1cecc703",
      "14a3438ecab94c95b3cfd8cc175b1677",
      "a2ec00e871d74b8db61f340e84e49e25",
      "d3764b93d8f74989867c52852d1d6f4d",
      "f3424efa869d4b579e2cba89e69e4976",
      "114a2b3336e948899bcb950d55ec4142",
      "2f47881ed5684234a5c97e0af7886c1d",
      "e277cfd7978247e59d0904dfd086c701",
      "a2af0b49f54c40a598fe53f0b7144c72",
      "c14ad6fad4324cd29cc6178f63a8cded",
      "f995edda20164c0b9d90a0b8059c99c4",
      "1111d705b090448cbace9c2c29ea93fa",
      "20ec443d74434922b645266130249a3a",
      "0dbd380aec4b4e7ab481d29066616723",
      "f734996feeea41f38413c8d50b8d7c40",
      "338dc261d2944b32aaff162e8881d269",
      "0ab2d3e9aee44fc2be4ee368ce38f9ac",
      "d7463c288c6742a59cb09541189bd068",
      "d327266c38624373b0483d704e8be178",
      "3a9fa0f70408469eb10938e1497e3b74",
      "6f49cc1902ab4d64b6363b274b324806",
      "e4557e8f3acb457da608871744be899c",
      "bf060d173342453fbad4d1af4949be18",
      "96633ab9838e460b8528eeef2854f46c",
      "b7d5792bc8014f2c9d664d2ab08fda3c",
      "cfddc859a8764139a8adc9d80227d4d3",
      "24fb51e2e9654c6e98f628a096b1aad0",
      "4b1bee5600b1499ca5d3ca4e190137fd",
      "572dc3481b1348578178bc7e52436374",
      "da3729683bf047b9a3ce0df58b9e6e2d",
      "c1ad03eb552145d086dca5650f7256e6",
      "28748337e6db437f81a8bc3cc7ccc218",
      "6954ca08b16343638408cf74a2c65d42",
      "9c0dafbf3b05446caa8774e7bb2d540e",
      "5751e9490ed94c92ac776a48656394e1",
      "4dd8ad050173478f9c55c506f786da9a",
      "90a05d16f45a4ea687373ec924d56c93",
      "2e62859f509948c494871e4f6e2177a6"
     ]
    },
    "id": "e5382431-497e-4688-b4ec-8958a92163e7",
    "outputId": "97407080-6b57-4cc3-e68c-459512f01a00",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e1ccb71ea640608c482a607fa0bda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d427d030c4240bea68d889b4e688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf354f6dba34cee8678115c6eeda106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401dcb308a484d56a7b85e97ac5a6dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877a36bafe0e4a24943a7e08e8361ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9022b23f18c0480b8ae23aa0b23089be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ced60d021f7486f8474d6e28f6b2c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1e04909dc74ac4b54cd3dd2a4e732b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da8f9c26dc54505aaa9b17652b0cba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602df16e24b34c2ab7cacfc515d1e892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216f21ccadc046339f1f6924273bc383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id.value)\n",
    "\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
    "pt_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d14b98-3d1a-483d-972b-aa32c58c87c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x. 4 1000740000 root 79 Jan 13 16:30 .\n",
      "drwxr-xr-x. 3 1000740000 root 51 Jan 13 16:30 ..\n",
      "drwxr-xr-x. 3 1000740000 root 46 Jan 13 16:30 .locks\n",
      "drwxr-xr-x. 6 1000740000 root 65 Jan 13  2025 models--openai--whisper-large-v2\n",
      "-rw-r--r--. 1 1000740000 root  1 Jan 13 16:30 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f957d-1f8c-4629-a21a-e0abaf34e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2033d404-00d8-41a1-bd89-2eb47aa7ea80",
   "metadata": {
    "id": "IRS8EXpwFZlR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "this_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "this_device\n",
    "print(f\"Using device: {this_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a6d7c8-71df-4307-ae53-98ad6569f6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a19cdc-ba91-4324-977d-1541deb7efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a34fe47a-ce0e-4405-aa91-8dc7d6f6c90b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: comment - pytorch doesnt have the tool\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78476-5d6b-40d2-9bb4-4df5768f400c",
   "metadata": {
    "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
   },
   "source": [
    "### Preparing the Input Sample\n",
    "\n",
    "To use the Whisper model for speech recognition, we need to properly prepare the input audio sample. The `WhisperProcessor` expects the audio data to be in the form of a NumPy array, along with information about the audio sampling rate. It then processes the audio and returns the `input_features` tensor, which is used for making predictions.\n",
    "\n",
    "The conversion of the audio file to the required NumPy format is conveniently handled by the Hugging Face Datasets library. This library provides a seamless interface for loading and preprocessing audio data, making it easier to integrate with the Whisper model.\n",
    "\n",
    "To prepare the input sample, the next Python code:\n",
    "\n",
    "1. Loads the audio file using the Hugging Face Datasets library.\n",
    "2. Extracts the audio data as a NumPy array and obtain the sampling rate.\n",
    "3. Passes the audio array and sampling rate to the `WhisperProcessor`.\n",
    "4. Retrieves the `input_features` tensor from the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cefde2-c1e9-4583-90cd-83303834f937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x. 4 1000740000 root 79 Jan 13 16:30 .\n",
      "drwxr-xr-x. 3 1000740000 root 51 Jan 13 16:30 ..\n",
      "drwxr-xr-x. 3 1000740000 root 46 Jan 13 16:30 .locks\n",
      "drwxr-xr-x. 6 1000740000 root 65 Jan 13 16:31 models--openai--whisper-large-v2\n",
      "-rw-r--r--. 1 1000740000 root  1 Jan 13 16:30 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e408ae6-d739-4436-8d22-5836345777e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/huggingface/datasets': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19440d7e-3427-469a-a418-8ff96aa85e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e94dd40e-4573-44fe-8703-3279673f7a7b",
   "metadata": {},
   "source": [
    "### dataset:\n",
    "    \"distil-whisper/librispeech_long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9b93df-a496-4847-bb19-c400d540d69b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "54f80c042875442cbc4886b33a7fed1b",
      "a763b3e0cf794b81b0e1d58dd6cd2fdc",
      "b8e96e211f96493f9bafdc9a8f8083fb",
      "3b29eaf965b74f8b8a928f7e1de00a71",
      "85126b4bc6c241b0b6ac6f0fc16d2fc4",
      "ddc61dc02f8c44da9dd1ce8f3828e379",
      "ef45cbd9e7744aae938b654ecb5eac11",
      "287c0aa4ae014130afdf66c79026f284",
      "0c17481318eb4f65a2e4afef66446916",
      "1c2c78b8560d4e84a3d8a63eb90e6a9f",
      "4e6c41945a704a579e6a455bb598cc56",
      "610b9c4c9e12491cb5833d3bc3dd300a",
      "063fe5fe1af74e6489b9f265b3541e09",
      "cdc1772e03d54b2c9d48a9e668156c4d",
      "4e00c89d62044e27aa5a37d8561d9a4d",
      "207b8969abf14a028dfd650b4378dd14",
      "0efcadf24d04404fb85c4d7a8556df4b",
      "0c5dc31513864bf995bda491fb83b0cf",
      "0b5ab99e70924d4681c35cca99eb26f6",
      "7ad382ac2fee4ca3bdd53a743dd7ca9b",
      "316fc74123eb498f9b867f35c6bde185",
      "5e426aa6112940db8de9017616f159dc",
      "473f60c52bf847a8bb2279b9633f913a",
      "47fa45bebf4c4659a77b258d980e37aa",
      "2dbca1333acc4e17af43239de903bbfd",
      "8edef4788f284f9d9d240dd927e391b4",
      "346e9644dd5b476db03fb322758142e9",
      "29c2e2ec675a485d89e70d1accb93eb1",
      "95e98ef53d4a4da194a3afd7e5a4e3d4",
      "ffe1e64502f44624b8462bb8b0882e51",
      "ea75c05a7f3749ff9cc0980470d4a70b",
      "9f6413789c284a2cafb73a2b46d2e261",
      "c167041d90184f0fb47646759bda9148"
     ]
    },
    "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111",
    "outputId": "03c77fb1-b50f-416c-949b-35cb3e61b2ab",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307cc0db94804c49bffbe05d5b345408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d29d47d2614760b1d8af0fc64acb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-913508124a40cb97.parquet:   0%|          | 0.00/1.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a0096a4b7649f7883c110647797269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_input_features(sample):\n",
    "    input_features = processor(\n",
    "        sample[\"audio\"][\"array\"],\n",
    "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"distil-whisper/librispeech_long\", \"clean\", split=\"validation\"\n",
    ")\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3562d156-6aa3-45f7-b91a-02dbf627da38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "         0.0005188 ]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa960c3f-36ba-4940-9fc6-26b8130f7d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1933e-01, -9.4576e-02, -1.0978e-01,  ...,  2.2357e-01,\n",
       "           2.7244e-01,  4.1318e-01],\n",
       "         [ 4.9347e-04, -8.9271e-02, -6.7290e-02,  ...,  4.2127e-01,\n",
       "           4.6272e-01,  4.1557e-01],\n",
       "         [-1.5326e-01, -2.0804e-01, -2.2227e-01,  ...,  8.5404e-01,\n",
       "           8.3540e-01,  8.4285e-01],\n",
       "         ...,\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -1.8856e-01,\n",
       "          -1.9991e-01, -4.2395e-02],\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -6.8221e-02,\n",
       "          -1.7072e-01, -4.4241e-02],\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -1.1030e-01,\n",
       "          -1.8742e-01, -5.2066e-02]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05427acb-f858-44b1-96a2-d302b861c230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x. 3 1000740000 root 192 Jan 13 16:32 .\n",
      "drwxr-xr-x. 4 1000740000 root  67 Jan 13 16:32 ..\n",
      "drwxr-xr-x. 3 1000740000 root  19 Jan 13 16:32 distil-whisper___librispeech_long\n",
      "-rw-r--r--. 1 1000740000 root   0 Jan 13 16:32 _opt_app-root_src_.cache_huggingface_datasets_distil-whisper___librispeech_long_clean_0.0.0_164d3b41852b1eebe89f1dc0e6e0042f16835ea0.lock\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e8759-47ee-434f-b3fc-61ffebf1b35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c25625-7462-4a3b-8ca8-77c65dd0dcc9",
   "metadata": {
    "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
   },
   "source": [
    "### Running Model Inference\n",
    "\n",
    "With the input sample prepared, we can now perform speech recognition using the Whisper model. The model provides a convenient `generate` interface that simplifies the inference process. Here's how you can run the model inference:\n",
    "\n",
    "1. Pass the `input_features` tensor to the `generate` method of the Whisper model.\n",
    "2. The model will process the input and generate the predicted token IDs.\n",
    "3. Once the generation is complete, use the `processor.batch_decode` method to decode the predicted token IDs into human-readable text transcription.\n",
    "\n",
    "The `generate` method handles the complex task of sequence generation, taking into account the model's architecture and the provided input features. It produces the predicted token IDs, which represent the transcribed text in a encoded format.\n",
    "\n",
    "By leveraging the `generate` interface and the `processor.batch_decode` method, you can easily perform speech recognition with the Whisper model. The model takes care of the complex task of mapping the audio input to text output, while the processor handles the necessary decoding step to provide you with the final transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51970a1-e107-43b1-b8e6-7ee62e0454d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd62840b-69a7-4dff-bca0-2226a28a87a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "c9618867-beae-4875-a5be-0e0a3b453414",
    "outputId": "7733e9a6-6f7a-417d-b4ee-1e99f2aec090",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 4s, sys: 22.3 s, total: 4min 26s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import IPython.display as ipd\n",
    "\n",
    "predicted_ids = pt_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "#display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "# aa: sample doesnt have \n",
    "#print(f\"Reference: {sample['text']}\")\n",
    "#print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21b488c1-fff0-40af-843e-4e7917929d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his manner. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover\n"
     ]
    }
   ],
   "source": [
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ee727d-eb01-4458-b388-512758c9707e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his manner. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb97e-3c2b-4a48-a648-1488fd30d127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08389b7-fe2e-4152-92cf-ceec0483051c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (24.3.1)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.48.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets[audio] in /opt/app-root/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets[audio]) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (3.9.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.13.0)\n",
      "Requirement already satisfied: librosa in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.10.2.post1)\n",
      "Requirement already satisfied: soxr>=0.4.0 in /opt/app-root/lib/python3.9/site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/app-root/lib/python3.9/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.28.3)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.6)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/app-root/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets[audio]) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets[audio]) (2024.1)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/app-root/lib/python3.9/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/app-root/lib/python3.9/site-packages (from pooch>=1.1->librosa->datasets[audio]) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "073272bb-1245-4f42-a059-a3e1c9a691fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#!pip install 'accelerate>=0.26.0'\n",
    "!pip install 'accelerate==0.27.0'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa1d8909-8021-4fa9-bec5-0dca0cbd0b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b16477d-5985-4cfe-b55f-436244637176",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "459f9037-7bcc-431f-895e-ff6606427303",
   "metadata": {
    "tags": []
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "from IPython.display import Audio as display_Audio, display\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c23bf1-16f4-4c48-a2e6-ad615dc91780",
   "metadata": {},
   "source": [
    "###  Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9526355-1428-49d5-b842-c6816324b50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a15ccfa2-f67f-4fe6-b715-30c6c04fa561",
   "metadata": {},
   "source": [
    "# TBD on GPU :\n",
    "   remove lines with #aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70615b8a-bb1a-49fb-b34b-d2f6773fd612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf27351f7c854073ba77741706a6430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddafdf09e28c4110be25a9a1cc5f8dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ffcdc7720e472ca5157a35a5dddb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87263c2b92a4761acde8b1d4259c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1da347aebe44927917465c58f15fb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb74222690046c297f32ca4c0e55614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f28568ea5d404da93c19cdb0d289e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0488201cfca47809c1ad732228c031b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5276763bf24796a8aaf62665d3a132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f766cc57c1d4798b417eaea06fc2a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c8104fb2d64df18606b3c8de068a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n",
      "CPU times: user 9min 33s, sys: 1min 43s, total: 11min 17s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    \n",
    "        max_new_tokens=128, # aa : added to make it work\n",
    "        chunk_length_s=30,  # aa\n",
    "        batch_size=16,        # aa\n",
    "        return_timestamps=True  # aa\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edc3185d-5c3e-4536-a4d0-66b23a9c9a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#https://www.intel.com/content/www/us/en/developer/articles/technical/speech-recognition-in-openai-whisper-without-a-gpu.html\n",
    "#utility functions\n",
    "def load_recorded_audio(path_audio,input_sample_rate=48000,output_sample_rate=16000):\n",
    "    # Dataset: convert recorded audio to vector\n",
    "    waveform, sample_rate = torchaudio.load(path_audio)\n",
    "    waveform_resampled = torchaudio.functional.resample(waveform, orig_freq=input_sample_rate, new_freq=output_sample_rate) #change sample rate to 16000 to match training.\n",
    "    sample = waveform_resampled.numpy()[0]\n",
    "    return sample\n",
    "def run_inference(path_audio, output_lang, pipe):\n",
    "    sample = load_recorded_audio(path_audio)\n",
    "    result = pipe(sample, generate_kwargs = {\"language\": output_lang, \"task\": \"transcribe\"})\n",
    "    print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa7033ca-9780-4f0d-8807-abe117a5499c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "path_audio = \"example_1.wav\"\n",
    "#result = pipe(\"./example_1.wav\")\n",
    "output_lang = \"en\"\n",
    "run_inference(path_audio,output_lang, pipe)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eefc448-dc61-45eb-a1cd-f5754bc740f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "generate_kwargs = {\n",
    "        \"max_new_tokens\": 448,\n",
    "        \"num_beams\": 1,\n",
    "        \"condition_on_prev_tokens\": False,\n",
    "        \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n",
    "        \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "        \"logprob_threshold\": -1.0,\n",
    "        \"no_speech_threshold\": 0.6,\n",
    "        \"return_timestamps\": True,\n",
    "}\n",
    "\n",
    "result = pipe(sample, generate_kwargs=generate_kwargs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14c33b41-5eff-4c64-97c4-7bf8fb5d0f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "result = pipe(sample, generate_kwargs={\"language\": \"english\"})\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5a89494-5859-43eb-afd5-a6e01a95b4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "\n",
    "result = pipe(sample, return_timestamps=True)\n",
    "print(result[\"chunks\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "450f7f10-6387-4e8b-9e5a-d080c44a1279",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "result = pipe(sample, return_timestamps=\"word\")\n",
    "print(result[\"chunks\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82427c-9b22-4449-ac20-cfa4e047a3e1",
   "metadata": {},
   "source": [
    " ### Chunked Long-Form "
   ]
  },
  {
   "cell_type": "raw",
   "id": "28b9552e-4006-4530-978c-67360b404675",
   "metadata": {},
   "source": [
    "# aa TBD : with GPU, use low_cpu_mem_usage=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3ba4ad-0f4b-4f9c-b770-83d7581d78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n",
      "CPU times: user 7min 36s, sys: 30.4 s, total: 8min 6s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# aa : low_cpu_mem_usage from True to False\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,  # batch size for inference - set based on your device\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658623-0581-4daa-9bae-1a61e4bb8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4cd7a15-2fd8-40f1-b76a-64f690d59fae",
   "metadata": {},
   "source": [
    "### Torch compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7324c81-2ad4-4ed2-940b-25552391630e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9062279f-5cc9-4c19-bfc9-a16c8c1153c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesn't work\n",
    "%%time\n",
    "import torch\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# Enable static cache and compile the forward pass\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "# 2 warmup steps\n",
    "for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
    "        with sdpa_kernel(SDPBackend.MATH):\n",
    "                    result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n",
    "                \n",
    "# fast run\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "   result = pipe(sample.copy())\n",
    "                        \n",
    "   print(result[\"text\"])\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395a621-5e47-4562-86d4-73067965f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7011754b-950f-4fb4-a752-3ac0d50ba4df",
   "metadata": {},
   "source": [
    "###  Flash Attention 2 \n",
    "    to be tested on GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7a78df3-3f70-4bd0-ae8d-3a963e88757f",
   "metadata": {
    "tags": []
   },
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a16da50-866f-4d39-b43b-7ec1255cdba2",
   "metadata": {},
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a60fa6-b3c8-4edb-b827-51a085abb182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e31e4640-98ec-4524-9dde-77a643bad12f",
   "metadata": {},
   "source": [
    "###  Torch Scale-Product-Attention (SDPA) \n",
    "    to be tested on GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1739b771-45d8-4cfa-8e29-78ef037274a7",
   "metadata": {},
   "source": [
    "from transformers.utils import is_torch_sdpa_available\n",
    "\n",
    "print(is_torch_sdpa_available())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7460d478-339c-4a92-b82e-372d60194d2c",
   "metadata": {},
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4374ae-d145-4273-886e-7a07a94be183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1669f-bbd4-47e6-bd1d-d870285dd1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
