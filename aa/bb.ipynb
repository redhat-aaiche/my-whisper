{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d9bd72-189b-42c0-8f94-eb698ee3055e",
   "metadata": {},
   "source": [
    "Grettings to :\n",
    "https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb511-8b73-4053-a194-96a737191b55",
   "metadata": {
    "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the tutorial, ensure that you have the necessary prerequisites in place. This includes authenticating with the Hugging Face Hub using your token and verifying the authentication by running the provided code cells. These steps are crucial for accessing the required models and datasets throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9d482-02fc-46b5-852d-22f3f7178f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31185b55-3933-4760-9d35-a8c551aa1726",
   "metadata": {
    "id": "VCEKs-Y4wAYQ"
   },
   "source": [
    "## Setting up a Hugging Face account\n",
    "\n",
    "To access and utilize the vast array of datasets and machine learning models available on Hugging Face, an account is required. A Hugging Face account offers not just access to datasets like \"PolyAI/minds14\" but also allows for collaboration on projects, contribution to the community through dataset/model sharing, and even tracking progress on various machine learning tasks. Follow the steps outlined above to create your account and get started with Hugging Face.\n",
    "\n",
    "## To create a Hugging Face account, you can follow these simple steps:\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_join.png\" width=250>\n",
    "\n",
    "1. Go to the Hugging Face website.\n",
    "2. Click on “Sign Up” or go to https://huggingface.co/join\n",
    "3. Enter your email address and password (you can skip the prompt to \"Join organization\").\n",
    "4. If you already have an account, click on “Log in”.\n",
    "5. To interact with the Hub, you need to be logged in with a Hugging Face account.\n",
    "\n",
    "## To create a Hugging Face Access Token\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_settings_menu_option.png\" width=650>\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_create_token.png\" width=250>\n",
    "\n",
    "1. Go to the Access Tokens tab in your Hugging Face settings.\n",
    "2. Click on the New token button to create a new User Access Token.\n",
    "3. Select the `write` role and a name for your token. This notebook only needs a `read` role; however, we will be writing to Hugging Face in [Chapter 4](https://colab.research.google.com/drive/1LADNomT0JUBCsopU6r_NsZfOaNiz2N3h).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17071a18-2bea-4a0e-afcb-5aff8bb09a15",
   "metadata": {
    "id": "CFK6tOV0bLnE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628abc068ed24588bdb3acd1c292d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367e55be-f67a-47c7-9333-8be91bbf91ae",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '676187706ce06148154eb2f8',\n",
       " 'name': 'aaiche',\n",
       " 'fullname': 'aaiche',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/1bf3857a4d2a4e5e6fb5381ba1ed7752.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'AA_TOKEN',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-01-11T15:38:55.885Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '676187706ce06148154eb2f8',\n",
       "       'type': 'user',\n",
       "       'name': 'aaiche'},\n",
       "      'permissions': ['repo.content.read']}]}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify authentication\n",
    "from huggingface_hub import whoami\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15866060-1d89-44b5-b40a-77a60484fa3b",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd604-9abe-4fd1-bb9c-76d099a39cba",
   "metadata": {
    "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
   },
   "source": [
    "## Loading the PyTorch model\n",
    "\n",
    "Loading the PyTorch Whisper model is a straightforward process using the transformers library. The `AutoModelForSpeechSeq2Seq.from_pretrained` method is employed to initialize the model. In this tutorial, we will use the `distil-whisper/distil-large-v2` model as the default example. Please note that the model will be downloaded during the first run, which may take some time.\n",
    "\n",
    "However, you have the flexibility to choose from a variety of models available in the [Distil-Whisper Hugging Face collection](https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6). Some alternative options include `distil-whisper/distil-medium.en` and `distil-whisper/distil-small.en`. Additionally, models of the original Whisper architecture are also accessible, which you can explore further [here](https://huggingface.co/openai).\n",
    "\n",
    "It's important to highlight the significance of preprocessing and post-processing in the model's usage. The `AutoProcessor` class, specifically the `WhisperProcessor`, plays a crucial role in preparing the audio input data for the model. It handles tasks such as converting the audio to a Mel-spectrogram representation and decoding the predicted output token IDs back into a string using the tokenizer.\n",
    "\n",
    "To ensure a smooth and efficient workflow, the `AutoProcessor` class streamlines the preprocessing and post-processing steps, allowing you to focus on the core functionality of the Whisper model. By leveraging this class, you can easily integrate the Whisper model into your speech recognition pipeline, regardless of the specific model variant you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9518b369-cf8d-426d-b034-c89e14f6e39a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fee4dab4af144c69977cd71bd00c594c",
      "5f252a0c901040bc8f4850f3f2013109",
      "f872a9052ace4694b411dc6981d4a1df"
     ]
    },
    "id": "uqh7J71HNsYG",
    "outputId": "16413215-4b75-4601-c0db-26c2005796eb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e169c90fd1094f3398840fd5debb4c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model type:', index=1, options=('Distil-Whisper', 'Whisper'), value='Whisper')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = {\n",
    "    \"Distil-Whisper\": [\n",
    "        \"distil-whisper/distil-large-v2\",\n",
    "        \"distil-whisper/distil-medium.en\",\n",
    "        \"distil-whisper/distil-small.en\"\n",
    "    ],\n",
    "    \"Whisper\": [\n",
    "        \"openai/whisper-large-v3\",\n",
    "        \"openai/whisper-large-v2\",\n",
    "        \"openai/whisper-large\",\n",
    "        \"openai/whisper-medium\",\n",
    "        \"openai/whisper-small\",\n",
    "        \"openai/whisper-base\",\n",
    "        \"openai/whisper-tiny\",\n",
    "        \"openai/whisper-medium.en\",\n",
    "        \"openai/whisper-small.en\",\n",
    "        \"openai/whisper-base.en\",\n",
    "        \"openai/whisper-tiny.en\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_type = widgets.Dropdown(\n",
    "    options=model_ids.keys(),\n",
    "    value=\"Whisper\",\n",
    "    description=\"Model type:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f47a3b8-03f3-4dde-a5dd-267091952e5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18906804c7dc46cfa8e3b1428349d4e8",
      "522feef58e8e4804b610649249bddcf8",
      "fc63388bc1bf45f2bc817f2e408ab507"
     ]
    },
    "id": "WHkwWcsrNsYG",
    "outputId": "e7203767-d08d-4091-cee1-ecf282d950fe",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2124e38b77b4646b269b343ee36bc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('openai/whisper-large-v3', 'openai/whisper-large-v2', 'openai…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids[model_type.value],\n",
    "    value=model_ids[model_type.value][1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db265ac1-dc18-48cd-8d9b-772484c3fee0",
   "metadata": {
    "id": "zilL_lNDSRJ4"
   },
   "source": [
    "## Step 1: Loading the Transformers ASR Model\n",
    "\n",
    "To begin building our speech recognition demo, we first need to have an Automatic Speech Recognition (ASR) model. You can either train your own model or use a pre-trained one. In this tutorial, we will leverage a pre-trained ASR model called \"whisper\" from OpenAI.\n",
    "\n",
    "Loading the \"whisper\" model from the Hugging Face Transformers library is a straightforward process. Here's the code snippet to accomplish this:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "p = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "```\n",
    "\n",
    "With just these two lines of code, we initialize a pipeline for automatic speech recognition using the \"openai/whisper-base.en\" model. The pipeline abstracts away the complexities of working with the model directly, providing a high-level interface for performing ASR tasks.\n",
    "\n",
    "By utilizing a pre-trained model like \"whisper\", we can quickly get started with building our demo without the need for extensive model training. This allows us to focus on integrating the model into our application and creating an engaging user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176271-c4cb-46e7-9ae9-ed5a1bd33588",
   "metadata": {
    "id": "Q6F2DkegPQ1h"
   },
   "source": [
    "## Step 2: Building a Full-Context ASR Demo with Transformers\n",
    "\n",
    "Our first step in creating the speech recognition demo is to build a *full-context* ASR demo. In this demo, the user will speak the entire audio before the ASR model processes it and generates the transcription. Thanks to Gradio's intuitive interface, building this demo is a breeze.\n",
    "\n",
    "We'll start by creating a function that wraps around the `pipeline` object we initialized earlier. This function will serve as the core of our demo, handling the audio input and generating the transcription.\n",
    "\n",
    "To capture the user's audio input, we'll utilize Gradio's built-in `Audio` component. This component will be configured to accept input from the user's microphone and return the filepath of the recorded audio. For displaying the transcribed text, we'll use a simple `Textbox` component.\n",
    "\n",
    "The `transcribe` function, which is the heart of our demo, takes a single parameter called `audio`. This parameter represents the audio data recorded by the user, stored as a NumPy array. However, the `pipeline` object expects the audio data to be in the `float32` format. To ensure compatibility, we first convert the audio data to `float32` and then normalize it by dividing it by its maximum absolute value. Finally, we pass the processed audio data to the `pipeline` object to obtain the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e9e778-9529-4e77-9c23-d142416acaef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f32dae2-5b2a-4c41-9558-3f2775b22122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a369721-4b1b-4393-a879-50d33e533f20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "3d336c21e2c94e11999300d5be080c93",
      "017752c1241b42adb5588c9a17c6f720",
      "9723e5270fa54cb0aaacaa5727e78b82",
      "9b8923adfea54139bbe4704dab6700e0",
      "f9abec2dc1e1461d8737938d1b656eff",
      "9b2048a14c5f4a15a240de505db0ef71",
      "66b7c77d1ea7449dbc536654ae3a19e9",
      "46bdd548eb364f0dbc8404474ccd5c6e",
      "044d24485c6f40debcce50f46d999806",
      "df30d496022a43c684d0098deefdc7d8",
      "e1397cce85524490bafcce0949535c9e",
      "6709089d0b264cc7a53e592a8849ae4e",
      "413749954b7b4ca497b7d2152248ddbc",
      "da6535e37d9b454db1d1fd9c8143b426",
      "fc06df4071ab4d21899564b393270527",
      "15f2a143de9e4b1bb5b411eec29ef42a",
      "ffc68ab79c824f149dee353b6e84d06a",
      "17923d2aeaa84228beda6114210ed726",
      "9a37299d0e72431284e7c6d15b7645c2",
      "8a5219c49563466db82ea06f31d7a862",
      "a882380256e647fb8f881f4c2857b29a",
      "8a6810a87cf54deba34e6bc17d76bb77",
      "f53be024380c42b6949cc9ce29b69541",
      "b4183b280d8d4276850267beba60d6eb",
      "5761d7aa32e34665b0f05a1d57c50164",
      "b629cf2d8ced4d0c966ef50f95a31f50",
      "108c5e113f984cfaa6c84ca0b5672944",
      "8c61efd854a34bebba33725a49996716",
      "f4f9647247ac421186cbefe79a2ceb8c",
      "fce8c66829354cddbeaa5d56de2594e2",
      "58cb82243a9440558fe6acd0b9e3c043",
      "046716c66572443e8224874b54eed3b3",
      "4254e413534e4b1cada2779f5be838e6",
      "feb0b610d1eb4440835239b10dc02a59",
      "4e590a2b27b74343bf6b6004a7c2ed2d",
      "091d22a6af054115a33ccb7b1fde2417",
      "3f9addd6893c42e9b288ef8738dbb38c",
      "70f0e385ff084155a156089b39100c10",
      "07f9e3adc1c74275943cd6d9b22266b6",
      "ee877da8550945db81953805f778f4c4",
      "819c41022bec4d779e6665c2d28900ef",
      "0460068f54224027ad89207af574ab30",
      "fa55d5e1288b4caab46c025da64d9ecf",
      "c02b061234cb4659bc7f784a41a111db",
      "30e0ddc8c03b4ad8bc33f356c66c964b",
      "30fb46e0ea0d485ea6e5540306b74412",
      "0c4cef46a2074bbabb4254abe14118a6",
      "48c080cd107a49068e38ce9c9ff7152b",
      "abd6cc02f9134fb989492ebdbc2a65b1",
      "1aa7a2221b9440a0a5b23ce66b83f4ef",
      "e6d0fe0ca1af453b8e685d4026cf7b2e",
      "17f02827fc1a4d8aa26dfffc759f504b",
      "ab88a20e95f84ffbb188ebe56f8677fd",
      "e36d5f8d051e42e28a06e4e48d28ac39",
      "e688b7cca32a44f39dc08a360c538fa6",
      "d6b82a8c30164cd2b3b82ef58a0e8b49",
      "e7eab28a118146c08d1f5a14286b9f00",
      "ebc58f9829d147718e8f7362dc491a39",
      "b4c19b0475e541cd98ec68bd91939696",
      "261ff4d16895488095543db10f3abe4f",
      "1d8d4c53e6c6435da57f2b57e7503396",
      "292176279ece40019e3104d5779d3fbf",
      "426d4cdafb484ca9bbd189c3b9995c92",
      "3e321f2163a341c7acf349b4d7feb6c2",
      "d6785dd1ab3745f380cf8e1a846434fd",
      "50401796c2334720abf382399db3d280",
      "5637690ab3b646e3a9bd63683bedce40",
      "1eab1176f2ea420988bb9ba9e3b9263e",
      "369b51e6c05c4b8690d08114038fff42",
      "513b8d0fb9b94f088d010d0e358ac80b",
      "2ebcbbdd506a466aa7ff65eb52c8a8e1",
      "467fea05a7054eab8ec6caec0f093290",
      "ccab9b9588ed4d8bb8f47f5b4ddbed3e",
      "64be9f5414aa4510813e3ff7bd912261",
      "19ba69de75ae4df283737cb75534563d",
      "934997d106344543acf286795bc30c6b",
      "81a7a3bb1117401d947068d0d2a79332",
      "534241a659b942d09f664816b8db06ab",
      "692897f7a6db42f6a206f470b474a353",
      "9eb90aacb8fa4a689bc6cd3e3dc606e3",
      "55ce55d4429c416ba0ea8258f71dcaa2",
      "008eabbba79344f9887eae5a274c01cd",
      "bc4248579eb14f87b1b85a07feb3e02a",
      "ac8ccc097ca849dcb9d1d9bc1cecc703",
      "14a3438ecab94c95b3cfd8cc175b1677",
      "a2ec00e871d74b8db61f340e84e49e25",
      "d3764b93d8f74989867c52852d1d6f4d",
      "f3424efa869d4b579e2cba89e69e4976",
      "114a2b3336e948899bcb950d55ec4142",
      "2f47881ed5684234a5c97e0af7886c1d",
      "e277cfd7978247e59d0904dfd086c701",
      "a2af0b49f54c40a598fe53f0b7144c72",
      "c14ad6fad4324cd29cc6178f63a8cded",
      "f995edda20164c0b9d90a0b8059c99c4",
      "1111d705b090448cbace9c2c29ea93fa",
      "20ec443d74434922b645266130249a3a",
      "0dbd380aec4b4e7ab481d29066616723",
      "f734996feeea41f38413c8d50b8d7c40",
      "338dc261d2944b32aaff162e8881d269",
      "0ab2d3e9aee44fc2be4ee368ce38f9ac",
      "d7463c288c6742a59cb09541189bd068",
      "d327266c38624373b0483d704e8be178",
      "3a9fa0f70408469eb10938e1497e3b74",
      "6f49cc1902ab4d64b6363b274b324806",
      "e4557e8f3acb457da608871744be899c",
      "bf060d173342453fbad4d1af4949be18",
      "96633ab9838e460b8528eeef2854f46c",
      "b7d5792bc8014f2c9d664d2ab08fda3c",
      "cfddc859a8764139a8adc9d80227d4d3",
      "24fb51e2e9654c6e98f628a096b1aad0",
      "4b1bee5600b1499ca5d3ca4e190137fd",
      "572dc3481b1348578178bc7e52436374",
      "da3729683bf047b9a3ce0df58b9e6e2d",
      "c1ad03eb552145d086dca5650f7256e6",
      "28748337e6db437f81a8bc3cc7ccc218",
      "6954ca08b16343638408cf74a2c65d42",
      "9c0dafbf3b05446caa8774e7bb2d540e",
      "5751e9490ed94c92ac776a48656394e1",
      "4dd8ad050173478f9c55c506f786da9a",
      "90a05d16f45a4ea687373ec924d56c93",
      "2e62859f509948c494871e4f6e2177a6"
     ]
    },
    "id": "e5382431-497e-4688-b4ec-8958a92163e7",
    "outputId": "97407080-6b57-4cc3-e68c-459512f01a00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id.value)\n",
    "\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
    "pt_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d14b98-3d1a-483d-972b-aa32c58c87c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f957d-1f8c-4629-a21a-e0abaf34e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2033d404-00d8-41a1-bd89-2eb47aa7ea80",
   "metadata": {
    "id": "IRS8EXpwFZlR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "this_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "this_device\n",
    "print(f\"Using device: {this_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a6d7c8-71df-4307-ae53-98ad6569f6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a19cdc-ba91-4324-977d-1541deb7efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a34fe47a-ce0e-4405-aa91-8dc7d6f6c90b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: comment - pytorch doesnt have the tool\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78476-5d6b-40d2-9bb4-4df5768f400c",
   "metadata": {
    "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
   },
   "source": [
    "### Preparing the Input Sample\n",
    "\n",
    "To use the Whisper model for speech recognition, we need to properly prepare the input audio sample. The `WhisperProcessor` expects the audio data to be in the form of a NumPy array, along with information about the audio sampling rate. It then processes the audio and returns the `input_features` tensor, which is used for making predictions.\n",
    "\n",
    "The conversion of the audio file to the required NumPy format is conveniently handled by the Hugging Face Datasets library. This library provides a seamless interface for loading and preprocessing audio data, making it easier to integrate with the Whisper model.\n",
    "\n",
    "To prepare the input sample, the next Python code:\n",
    "\n",
    "1. Loads the audio file using the Hugging Face Datasets library.\n",
    "2. Extracts the audio data as a NumPy array and obtain the sampling rate.\n",
    "3. Passes the audio array and sampling rate to the `WhisperProcessor`.\n",
    "4. Retrieves the `input_features` tensor from the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72cefde2-c1e9-4583-90cd-83303834f937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e408ae6-d739-4436-8d22-5836345777e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19440d7e-3427-469a-a418-8ff96aa85e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e94dd40e-4573-44fe-8703-3279673f7a7b",
   "metadata": {},
   "source": [
    "### dataset:\n",
    "    \"distil-whisper/librispeech_long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9b93df-a496-4847-bb19-c400d540d69b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "54f80c042875442cbc4886b33a7fed1b",
      "a763b3e0cf794b81b0e1d58dd6cd2fdc",
      "b8e96e211f96493f9bafdc9a8f8083fb",
      "3b29eaf965b74f8b8a928f7e1de00a71",
      "85126b4bc6c241b0b6ac6f0fc16d2fc4",
      "ddc61dc02f8c44da9dd1ce8f3828e379",
      "ef45cbd9e7744aae938b654ecb5eac11",
      "287c0aa4ae014130afdf66c79026f284",
      "0c17481318eb4f65a2e4afef66446916",
      "1c2c78b8560d4e84a3d8a63eb90e6a9f",
      "4e6c41945a704a579e6a455bb598cc56",
      "610b9c4c9e12491cb5833d3bc3dd300a",
      "063fe5fe1af74e6489b9f265b3541e09",
      "cdc1772e03d54b2c9d48a9e668156c4d",
      "4e00c89d62044e27aa5a37d8561d9a4d",
      "207b8969abf14a028dfd650b4378dd14",
      "0efcadf24d04404fb85c4d7a8556df4b",
      "0c5dc31513864bf995bda491fb83b0cf",
      "0b5ab99e70924d4681c35cca99eb26f6",
      "7ad382ac2fee4ca3bdd53a743dd7ca9b",
      "316fc74123eb498f9b867f35c6bde185",
      "5e426aa6112940db8de9017616f159dc",
      "473f60c52bf847a8bb2279b9633f913a",
      "47fa45bebf4c4659a77b258d980e37aa",
      "2dbca1333acc4e17af43239de903bbfd",
      "8edef4788f284f9d9d240dd927e391b4",
      "346e9644dd5b476db03fb322758142e9",
      "29c2e2ec675a485d89e70d1accb93eb1",
      "95e98ef53d4a4da194a3afd7e5a4e3d4",
      "ffe1e64502f44624b8462bb8b0882e51",
      "ea75c05a7f3749ff9cc0980470d4a70b",
      "9f6413789c284a2cafb73a2b46d2e261",
      "c167041d90184f0fb47646759bda9148"
     ]
    },
    "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111",
    "outputId": "03c77fb1-b50f-416c-949b-35cb3e61b2ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_input_features(sample):\n",
    "    input_features = processor(\n",
    "        sample[\"audio\"][\"array\"],\n",
    "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"distil-whisper/librispeech_long\", \"clean\", split=\"validation\"\n",
    ")\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3562d156-6aa3-45f7-b91a-02dbf627da38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "         0.0005188 ]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa960c3f-36ba-4940-9fc6-26b8130f7d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1933e-01, -9.4576e-02, -1.0978e-01,  ...,  2.2357e-01,\n",
       "           2.7244e-01,  4.1318e-01],\n",
       "         [ 4.9347e-04, -8.9271e-02, -6.7290e-02,  ...,  4.2127e-01,\n",
       "           4.6272e-01,  4.1557e-01],\n",
       "         [-1.5326e-01, -2.0804e-01, -2.2227e-01,  ...,  8.5404e-01,\n",
       "           8.3540e-01,  8.4285e-01],\n",
       "         ...,\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -1.8856e-01,\n",
       "          -1.9991e-01, -4.2395e-02],\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -6.8221e-02,\n",
       "          -1.7072e-01, -4.4241e-02],\n",
       "         [-6.3673e-01, -6.3673e-01, -6.3673e-01,  ..., -1.1030e-01,\n",
       "          -1.8742e-01, -5.2066e-02]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05427acb-f858-44b1-96a2-d302b861c230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e8759-47ee-434f-b3fc-61ffebf1b35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c25625-7462-4a3b-8ca8-77c65dd0dcc9",
   "metadata": {
    "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
   },
   "source": [
    "### Running Model Inference\n",
    "\n",
    "With the input sample prepared, we can now perform speech recognition using the Whisper model. The model provides a convenient `generate` interface that simplifies the inference process. Here's how you can run the model inference:\n",
    "\n",
    "1. Pass the `input_features` tensor to the `generate` method of the Whisper model.\n",
    "2. The model will process the input and generate the predicted token IDs.\n",
    "3. Once the generation is complete, use the `processor.batch_decode` method to decode the predicted token IDs into human-readable text transcription.\n",
    "\n",
    "The `generate` method handles the complex task of sequence generation, taking into account the model's architecture and the provided input features. It produces the predicted token IDs, which represent the transcribed text in a encoded format.\n",
    "\n",
    "By leveraging the `generate` interface and the `processor.batch_decode` method, you can easily perform speech recognition with the Whisper model. The model takes care of the complex task of mapping the audio input to text output, while the processor handles the necessary decoding step to provide you with the final transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e51970a1-e107-43b1-b8e6-7ee62e0454d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd62840b-69a7-4dff-bca0-2226a28a87a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "c9618867-beae-4875-a5be-0e0a3b453414",
    "outputId": "7733e9a6-6f7a-417d-b4ee-1e99f2aec090",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import IPython.display as ipd\n",
    "\n",
    "predicted_ids = pt_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "#display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "# aa: sample doesnt have \n",
    "#print(f\"Reference: {sample['text']}\")\n",
    "#print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21b488c1-fff0-40af-843e-4e7917929d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40ee727d-eb01-4458-b388-512758c9707e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his manner. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb97e-3c2b-4a48-a648-1488fd30d127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a08389b7-fe2e-4152-92cf-ceec0483051c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "073272bb-1245-4f42-a059-a3e1c9a691fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#!pip install 'accelerate>=0.26.0'\n",
    "!pip install 'accelerate==0.27.0'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa1d8909-8021-4fa9-bec5-0dca0cbd0b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b16477d-5985-4cfe-b55f-436244637176",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "459f9037-7bcc-431f-895e-ff6606427303",
   "metadata": {
    "tags": []
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "from IPython.display import Audio as display_Audio, display\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c23bf1-16f4-4c48-a2e6-ad615dc91780",
   "metadata": {},
   "source": [
    "###  Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9526355-1428-49d5-b842-c6816324b50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a15ccfa2-f67f-4fe6-b715-30c6c04fa561",
   "metadata": {},
   "source": [
    "# TBD on GPU :\n",
    "   remove lines with #aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70615b8a-bb1a-49fb-b34b-d2f6773fd612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n",
      "CPU times: user 9min 52s, sys: 41.9 s, total: 10min 34s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    \n",
    "        max_new_tokens=128, # aa : added to make it work\n",
    "        chunk_length_s=30,  # aa\n",
    "        batch_size=16,        # aa\n",
    "        return_timestamps=True  # aa\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edc3185d-5c3e-4536-a4d0-66b23a9c9a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#https://www.intel.com/content/www/us/en/developer/articles/technical/speech-recognition-in-openai-whisper-without-a-gpu.html\n",
    "#utility functions\n",
    "def load_recorded_audio(path_audio,input_sample_rate=48000,output_sample_rate=16000):\n",
    "    # Dataset: convert recorded audio to vector\n",
    "    waveform, sample_rate = torchaudio.load(path_audio)\n",
    "    waveform_resampled = torchaudio.functional.resample(waveform, orig_freq=input_sample_rate, new_freq=output_sample_rate) #change sample rate to 16000 to match training.\n",
    "    sample = waveform_resampled.numpy()[0]\n",
    "    return sample\n",
    "def run_inference(path_audio, output_lang, pipe):\n",
    "    sample = load_recorded_audio(path_audio)\n",
    "    result = pipe(sample, generate_kwargs = {\"language\": output_lang, \"task\": \"transcribe\"})\n",
    "    print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa7033ca-9780-4f0d-8807-abe117a5499c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "path_audio = \"example_1.wav\"\n",
    "#result = pipe(\"./example_1.wav\")\n",
    "output_lang = \"en\"\n",
    "run_inference(path_audio,output_lang, pipe)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eefc448-dc61-45eb-a1cd-f5754bc740f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "generate_kwargs = {\n",
    "        \"max_new_tokens\": 448,\n",
    "        \"num_beams\": 1,\n",
    "        \"condition_on_prev_tokens\": False,\n",
    "        \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n",
    "        \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "        \"logprob_threshold\": -1.0,\n",
    "        \"no_speech_threshold\": 0.6,\n",
    "        \"return_timestamps\": True,\n",
    "}\n",
    "\n",
    "result = pipe(sample, generate_kwargs=generate_kwargs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14c33b41-5eff-4c64-97c4-7bf8fb5d0f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "result = pipe(sample, generate_kwargs={\"language\": \"english\"})\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5a89494-5859-43eb-afd5-a6e01a95b4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "\n",
    "result = pipe(sample, return_timestamps=True)\n",
    "print(result[\"chunks\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "450f7f10-6387-4e8b-9e5a-d080c44a1279",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesnt work\n",
    "result = pipe(sample, return_timestamps=\"word\")\n",
    "print(result[\"chunks\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82427c-9b22-4449-ac20-cfa4e047a3e1",
   "metadata": {},
   "source": [
    " ### Chunked Long-Form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3ba4ad-0f4b-4f9c-b770-83d7581d78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of Upguards and Adam paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampooer in a Turkish bath, Next man!\n",
      "CPU times: user 7min 44s, sys: 30.9 s, total: 8min 14s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,  # batch size for inference - set based on your device\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658623-0581-4daa-9bae-1a61e4bb8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4cd7a15-2fd8-40f1-b76a-64f690d59fae",
   "metadata": {},
   "source": [
    "### Torch compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7324c81-2ad4-4ed2-940b-25552391630e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9062279f-5cc9-4c19-bfc9-a16c8c1153c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: doesn't work\n",
    "%%time\n",
    "import torch\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# Enable static cache and compile the forward pass\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "# 2 warmup steps\n",
    "for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
    "        with sdpa_kernel(SDPBackend.MATH):\n",
    "                    result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n",
    "                \n",
    "# fast run\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "   result = pipe(sample.copy())\n",
    "                        \n",
    "   print(result[\"text\"])\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395a621-5e47-4562-86d4-73067965f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7011754b-950f-4fb4-a752-3ac0d50ba4df",
   "metadata": {},
   "source": [
    "###  Flash Attention 2 \n",
    "    to be tested on GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7a78df3-3f70-4bd0-ae8d-3a963e88757f",
   "metadata": {
    "tags": []
   },
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a16da50-866f-4d39-b43b-7ec1255cdba2",
   "metadata": {},
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a60fa6-b3c8-4edb-b827-51a085abb182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e31e4640-98ec-4524-9dde-77a643bad12f",
   "metadata": {},
   "source": [
    "###  Torch Scale-Product-Attention (SDPA) \n",
    "    to be tested on GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1739b771-45d8-4cfa-8e29-78ef037274a7",
   "metadata": {},
   "source": [
    "from transformers.utils import is_torch_sdpa_available\n",
    "\n",
    "print(is_torch_sdpa_available())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7460d478-339c-4a92-b82e-372d60194d2c",
   "metadata": {},
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4374ae-d145-4273-886e-7a07a94be183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
