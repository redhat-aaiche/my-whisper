{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9f184-d674-4d1b-a284-f82b3629eb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf64c8b9-100a-4bcc-80b1-0ea3a8ff4801",
   "metadata": {
    "id": "Nk-2j7nT-BUa"
   },
   "source": [
    "# RESTART\n",
    "![Restart_the_runtime_600x102.png](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter08/Restart_the_runtime_600x102.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cb511-8b73-4053-a194-96a737191b55",
   "metadata": {
    "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the tutorial, ensure that you have the necessary prerequisites in place. This includes authenticating with the Hugging Face Hub using your token and verifying the authentication by running the provided code cells. These steps are crucial for accessing the required models and datasets throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b9d482-02fc-46b5-852d-22f3f7178f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/app-root/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31185b55-3933-4760-9d35-a8c551aa1726",
   "metadata": {
    "id": "VCEKs-Y4wAYQ"
   },
   "source": [
    "## Setting up a Hugging Face account\n",
    "\n",
    "To access and utilize the vast array of datasets and machine learning models available on Hugging Face, an account is required. A Hugging Face account offers not just access to datasets like \"PolyAI/minds14\" but also allows for collaboration on projects, contribution to the community through dataset/model sharing, and even tracking progress on various machine learning tasks. Follow the steps outlined above to create your account and get started with Hugging Face.\n",
    "\n",
    "## To create a Hugging Face account, you can follow these simple steps:\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_join.png\" width=250>\n",
    "\n",
    "1. Go to the Hugging Face website.\n",
    "2. Click on “Sign Up” or go to https://huggingface.co/join\n",
    "3. Enter your email address and password (you can skip the prompt to \"Join organization\").\n",
    "4. If you already have an account, click on “Log in”.\n",
    "5. To interact with the Hub, you need to be logged in with a Hugging Face account.\n",
    "\n",
    "## To create a Hugging Face Access Token\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_settings_menu_option.png\" width=650>\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter03/hugging_face_create_token.png\" width=250>\n",
    "\n",
    "1. Go to the Access Tokens tab in your Hugging Face settings.\n",
    "2. Click on the New token button to create a new User Access Token.\n",
    "3. Select the `write` role and a name for your token. This notebook only needs a `read` role; however, we will be writing to Hugging Face in [Chapter 4](https://colab.research.google.com/drive/1LADNomT0JUBCsopU6r_NsZfOaNiz2N3h).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236922d-b2e0-46eb-910b-bad1ff9813c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b73a43-1342-40cf-814e-6ee8cbe7adac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17071a18-2bea-4a0e-afcb-5aff8bb09a15",
   "metadata": {
    "id": "CFK6tOV0bLnE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418f25458f07437f91a5caf60c2405ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e55be-f67a-47c7-9333-8be91bbf91ae",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify authentication\n",
    "from huggingface_hub import whoami\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15866060-1d89-44b5-b40a-77a60484fa3b",
   "metadata": {
    "id": "MMHkYq7oF6y4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd604-9abe-4fd1-bb9c-76d099a39cba",
   "metadata": {
    "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
   },
   "source": [
    "## Loading the PyTorch model\n",
    "\n",
    "Loading the PyTorch Whisper model is a straightforward process using the transformers library. The `AutoModelForSpeechSeq2Seq.from_pretrained` method is employed to initialize the model. In this tutorial, we will use the `distil-whisper/distil-large-v2` model as the default example. Please note that the model will be downloaded during the first run, which may take some time.\n",
    "\n",
    "However, you have the flexibility to choose from a variety of models available in the [Distil-Whisper Hugging Face collection](https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6). Some alternative options include `distil-whisper/distil-medium.en` and `distil-whisper/distil-small.en`. Additionally, models of the original Whisper architecture are also accessible, which you can explore further [here](https://huggingface.co/openai).\n",
    "\n",
    "It's important to highlight the significance of preprocessing and post-processing in the model's usage. The `AutoProcessor` class, specifically the `WhisperProcessor`, plays a crucial role in preparing the audio input data for the model. It handles tasks such as converting the audio to a Mel-spectrogram representation and decoding the predicted output token IDs back into a string using the tokenizer.\n",
    "\n",
    "To ensure a smooth and efficient workflow, the `AutoProcessor` class streamlines the preprocessing and post-processing steps, allowing you to focus on the core functionality of the Whisper model. By leveraging this class, you can easily integrate the Whisper model into your speech recognition pipeline, regardless of the specific model variant you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9518b369-cf8d-426d-b034-c89e14f6e39a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fee4dab4af144c69977cd71bd00c594c",
      "5f252a0c901040bc8f4850f3f2013109",
      "f872a9052ace4694b411dc6981d4a1df"
     ]
    },
    "id": "uqh7J71HNsYG",
    "outputId": "16413215-4b75-4601-c0db-26c2005796eb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d79675adc742b99aba9529015a9b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model type:', index=1, options=('Distil-Whisper', 'Whisper'), value='Whisper')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = {\n",
    "    \"Distil-Whisper\": [\n",
    "        \"distil-whisper/distil-large-v2\",\n",
    "        \"distil-whisper/distil-medium.en\",\n",
    "        \"distil-whisper/distil-small.en\"\n",
    "    ],\n",
    "    \"Whisper\": [\n",
    "        \"openai/whisper-large-v3\",\n",
    "        \"openai/whisper-large-v2\",\n",
    "        \"openai/whisper-large\",\n",
    "        \"openai/whisper-medium\",\n",
    "        \"openai/whisper-small\",\n",
    "        \"openai/whisper-base\",\n",
    "        \"openai/whisper-tiny\",\n",
    "        \"openai/whisper-medium.en\",\n",
    "        \"openai/whisper-small.en\",\n",
    "        \"openai/whisper-base.en\",\n",
    "        \"openai/whisper-tiny.en\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_type = widgets.Dropdown(\n",
    "    options=model_ids.keys(),\n",
    "    value=\"Whisper\",\n",
    "    description=\"Model type:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f47a3b8-03f3-4dde-a5dd-267091952e5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "18906804c7dc46cfa8e3b1428349d4e8",
      "522feef58e8e4804b610649249bddcf8",
      "fc63388bc1bf45f2bc817f2e408ab507"
     ]
    },
    "id": "WHkwWcsrNsYG",
    "outputId": "e7203767-d08d-4091-cee1-ecf282d950fe",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd25c13a7734022806121fe56f6c72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('openai/whisper-large-v3', 'openai/whisper-large-v2', 'openai…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids[model_type.value],\n",
    "    value=model_ids[model_type.value][1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db265ac1-dc18-48cd-8d9b-772484c3fee0",
   "metadata": {
    "id": "zilL_lNDSRJ4"
   },
   "source": [
    "## Step 1: Loading the Transformers ASR Model\n",
    "\n",
    "To begin building our speech recognition demo, we first need to have an Automatic Speech Recognition (ASR) model. You can either train your own model or use a pre-trained one. In this tutorial, we will leverage a pre-trained ASR model called \"whisper\" from OpenAI.\n",
    "\n",
    "Loading the \"whisper\" model from the Hugging Face Transformers library is a straightforward process. Here's the code snippet to accomplish this:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "p = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "```\n",
    "\n",
    "With just these two lines of code, we initialize a pipeline for automatic speech recognition using the \"openai/whisper-base.en\" model. The pipeline abstracts away the complexities of working with the model directly, providing a high-level interface for performing ASR tasks.\n",
    "\n",
    "By utilizing a pre-trained model like \"whisper\", we can quickly get started with building our demo without the need for extensive model training. This allows us to focus on integrating the model into our application and creating an engaging user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176271-c4cb-46e7-9ae9-ed5a1bd33588",
   "metadata": {
    "id": "Q6F2DkegPQ1h"
   },
   "source": [
    "## Step 2: Building a Full-Context ASR Demo with Transformers\n",
    "\n",
    "Our first step in creating the speech recognition demo is to build a *full-context* ASR demo. In this demo, the user will speak the entire audio before the ASR model processes it and generates the transcription. Thanks to Gradio's intuitive interface, building this demo is a breeze.\n",
    "\n",
    "We'll start by creating a function that wraps around the `pipeline` object we initialized earlier. This function will serve as the core of our demo, handling the audio input and generating the transcription.\n",
    "\n",
    "To capture the user's audio input, we'll utilize Gradio's built-in `Audio` component. This component will be configured to accept input from the user's microphone and return the filepath of the recorded audio. For displaying the transcribed text, we'll use a simple `Textbox` component.\n",
    "\n",
    "The `transcribe` function, which is the heart of our demo, takes a single parameter called `audio`. This parameter represents the audio data recorded by the user, stored as a NumPy array. However, the `pipeline` object expects the audio data to be in the `float32` format. To ensure compatibility, we first convert the audio data to `float32` and then normalize it by dividing it by its maximum absolute value. Finally, we pass the processed audio data to the `pipeline` object to obtain the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e9e778-9529-4e77-9c23-d142416acaef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x. 7 1000740000 root 4.0K Jan 13 10:38 .\n",
      "drwxr-xr-x. 4 1000740000 root   67 Jan 13 10:19 ..\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:38 datasets--distil-whisper--librispeech_long\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:18 datasets--hf-internal-testing--librispeech_asr_dummy\n",
      "drwxr-xr-x. 6 1000740000 root 4.0K Jan 13 10:38 .locks\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:20 models--distil-whisper--distil-medium.en\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:37 models--openai--whisper-large-v2\n",
      "-rw-r--r--. 1 1000740000 root    1 Jan 13 10:35 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a369721-4b1b-4393-a879-50d33e533f20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "3d336c21e2c94e11999300d5be080c93",
      "017752c1241b42adb5588c9a17c6f720",
      "9723e5270fa54cb0aaacaa5727e78b82",
      "9b8923adfea54139bbe4704dab6700e0",
      "f9abec2dc1e1461d8737938d1b656eff",
      "9b2048a14c5f4a15a240de505db0ef71",
      "66b7c77d1ea7449dbc536654ae3a19e9",
      "46bdd548eb364f0dbc8404474ccd5c6e",
      "044d24485c6f40debcce50f46d999806",
      "df30d496022a43c684d0098deefdc7d8",
      "e1397cce85524490bafcce0949535c9e",
      "6709089d0b264cc7a53e592a8849ae4e",
      "413749954b7b4ca497b7d2152248ddbc",
      "da6535e37d9b454db1d1fd9c8143b426",
      "fc06df4071ab4d21899564b393270527",
      "15f2a143de9e4b1bb5b411eec29ef42a",
      "ffc68ab79c824f149dee353b6e84d06a",
      "17923d2aeaa84228beda6114210ed726",
      "9a37299d0e72431284e7c6d15b7645c2",
      "8a5219c49563466db82ea06f31d7a862",
      "a882380256e647fb8f881f4c2857b29a",
      "8a6810a87cf54deba34e6bc17d76bb77",
      "f53be024380c42b6949cc9ce29b69541",
      "b4183b280d8d4276850267beba60d6eb",
      "5761d7aa32e34665b0f05a1d57c50164",
      "b629cf2d8ced4d0c966ef50f95a31f50",
      "108c5e113f984cfaa6c84ca0b5672944",
      "8c61efd854a34bebba33725a49996716",
      "f4f9647247ac421186cbefe79a2ceb8c",
      "fce8c66829354cddbeaa5d56de2594e2",
      "58cb82243a9440558fe6acd0b9e3c043",
      "046716c66572443e8224874b54eed3b3",
      "4254e413534e4b1cada2779f5be838e6",
      "feb0b610d1eb4440835239b10dc02a59",
      "4e590a2b27b74343bf6b6004a7c2ed2d",
      "091d22a6af054115a33ccb7b1fde2417",
      "3f9addd6893c42e9b288ef8738dbb38c",
      "70f0e385ff084155a156089b39100c10",
      "07f9e3adc1c74275943cd6d9b22266b6",
      "ee877da8550945db81953805f778f4c4",
      "819c41022bec4d779e6665c2d28900ef",
      "0460068f54224027ad89207af574ab30",
      "fa55d5e1288b4caab46c025da64d9ecf",
      "c02b061234cb4659bc7f784a41a111db",
      "30e0ddc8c03b4ad8bc33f356c66c964b",
      "30fb46e0ea0d485ea6e5540306b74412",
      "0c4cef46a2074bbabb4254abe14118a6",
      "48c080cd107a49068e38ce9c9ff7152b",
      "abd6cc02f9134fb989492ebdbc2a65b1",
      "1aa7a2221b9440a0a5b23ce66b83f4ef",
      "e6d0fe0ca1af453b8e685d4026cf7b2e",
      "17f02827fc1a4d8aa26dfffc759f504b",
      "ab88a20e95f84ffbb188ebe56f8677fd",
      "e36d5f8d051e42e28a06e4e48d28ac39",
      "e688b7cca32a44f39dc08a360c538fa6",
      "d6b82a8c30164cd2b3b82ef58a0e8b49",
      "e7eab28a118146c08d1f5a14286b9f00",
      "ebc58f9829d147718e8f7362dc491a39",
      "b4c19b0475e541cd98ec68bd91939696",
      "261ff4d16895488095543db10f3abe4f",
      "1d8d4c53e6c6435da57f2b57e7503396",
      "292176279ece40019e3104d5779d3fbf",
      "426d4cdafb484ca9bbd189c3b9995c92",
      "3e321f2163a341c7acf349b4d7feb6c2",
      "d6785dd1ab3745f380cf8e1a846434fd",
      "50401796c2334720abf382399db3d280",
      "5637690ab3b646e3a9bd63683bedce40",
      "1eab1176f2ea420988bb9ba9e3b9263e",
      "369b51e6c05c4b8690d08114038fff42",
      "513b8d0fb9b94f088d010d0e358ac80b",
      "2ebcbbdd506a466aa7ff65eb52c8a8e1",
      "467fea05a7054eab8ec6caec0f093290",
      "ccab9b9588ed4d8bb8f47f5b4ddbed3e",
      "64be9f5414aa4510813e3ff7bd912261",
      "19ba69de75ae4df283737cb75534563d",
      "934997d106344543acf286795bc30c6b",
      "81a7a3bb1117401d947068d0d2a79332",
      "534241a659b942d09f664816b8db06ab",
      "692897f7a6db42f6a206f470b474a353",
      "9eb90aacb8fa4a689bc6cd3e3dc606e3",
      "55ce55d4429c416ba0ea8258f71dcaa2",
      "008eabbba79344f9887eae5a274c01cd",
      "bc4248579eb14f87b1b85a07feb3e02a",
      "ac8ccc097ca849dcb9d1d9bc1cecc703",
      "14a3438ecab94c95b3cfd8cc175b1677",
      "a2ec00e871d74b8db61f340e84e49e25",
      "d3764b93d8f74989867c52852d1d6f4d",
      "f3424efa869d4b579e2cba89e69e4976",
      "114a2b3336e948899bcb950d55ec4142",
      "2f47881ed5684234a5c97e0af7886c1d",
      "e277cfd7978247e59d0904dfd086c701",
      "a2af0b49f54c40a598fe53f0b7144c72",
      "c14ad6fad4324cd29cc6178f63a8cded",
      "f995edda20164c0b9d90a0b8059c99c4",
      "1111d705b090448cbace9c2c29ea93fa",
      "20ec443d74434922b645266130249a3a",
      "0dbd380aec4b4e7ab481d29066616723",
      "f734996feeea41f38413c8d50b8d7c40",
      "338dc261d2944b32aaff162e8881d269",
      "0ab2d3e9aee44fc2be4ee368ce38f9ac",
      "d7463c288c6742a59cb09541189bd068",
      "d327266c38624373b0483d704e8be178",
      "3a9fa0f70408469eb10938e1497e3b74",
      "6f49cc1902ab4d64b6363b274b324806",
      "e4557e8f3acb457da608871744be899c",
      "bf060d173342453fbad4d1af4949be18",
      "96633ab9838e460b8528eeef2854f46c",
      "b7d5792bc8014f2c9d664d2ab08fda3c",
      "cfddc859a8764139a8adc9d80227d4d3",
      "24fb51e2e9654c6e98f628a096b1aad0",
      "4b1bee5600b1499ca5d3ca4e190137fd",
      "572dc3481b1348578178bc7e52436374",
      "da3729683bf047b9a3ce0df58b9e6e2d",
      "c1ad03eb552145d086dca5650f7256e6",
      "28748337e6db437f81a8bc3cc7ccc218",
      "6954ca08b16343638408cf74a2c65d42",
      "9c0dafbf3b05446caa8774e7bb2d540e",
      "5751e9490ed94c92ac776a48656394e1",
      "4dd8ad050173478f9c55c506f786da9a",
      "90a05d16f45a4ea687373ec924d56c93",
      "2e62859f509948c494871e4f6e2177a6"
     ]
    },
    "id": "e5382431-497e-4688-b4ec-8958a92163e7",
    "outputId": "97407080-6b57-4cc3-e68c-459512f01a00",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id.value)\n",
    "\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
    "pt_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d14b98-3d1a-483d-972b-aa32c58c87c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x. 7 1000740000 root 4.0K Jan 13 10:38 .\n",
      "drwxr-xr-x. 4 1000740000 root   67 Jan 13 10:19 ..\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:38 datasets--distil-whisper--librispeech_long\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:18 datasets--hf-internal-testing--librispeech_asr_dummy\n",
      "drwxr-xr-x. 6 1000740000 root 4.0K Jan 13 10:38 .locks\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:20 models--distil-whisper--distil-medium.en\n",
      "drwxr-xr-x. 6 1000740000 root   65 Jan 13 10:37 models--openai--whisper-large-v2\n",
      "-rw-r--r--. 1 1000740000 root    1 Jan 13 10:35 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f957d-1f8c-4629-a21a-e0abaf34e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2033d404-00d8-41a1-bd89-2eb47aa7ea80",
   "metadata": {
    "id": "IRS8EXpwFZlR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "this_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "this_device\n",
    "print(f\"Using device: {this_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a6d7c8-71df-4307-ae53-98ad6569f6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a19cdc-ba91-4324-977d-1541deb7efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a34fe47a-ce0e-4405-aa91-8dc7d6f6c90b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# aa: comment - pytorch doesnt have the tool\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78476-5d6b-40d2-9bb4-4df5768f400c",
   "metadata": {
    "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
   },
   "source": [
    "### Preparing the Input Sample\n",
    "\n",
    "To use the Whisper model for speech recognition, we need to properly prepare the input audio sample. The `WhisperProcessor` expects the audio data to be in the form of a NumPy array, along with information about the audio sampling rate. It then processes the audio and returns the `input_features` tensor, which is used for making predictions.\n",
    "\n",
    "The conversion of the audio file to the required NumPy format is conveniently handled by the Hugging Face Datasets library. This library provides a seamless interface for loading and preprocessing audio data, making it easier to integrate with the Whisper model.\n",
    "\n",
    "To prepare the input sample, the next Python code:\n",
    "\n",
    "1. Loads the audio file using the Hugging Face Datasets library.\n",
    "2. Extracts the audio data as a NumPy array and obtain the sampling rate.\n",
    "3. Passes the audio array and sampling rate to the `WhisperProcessor`.\n",
    "4. Retrieves the `input_features` tensor from the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cefde2-c1e9-4583-90cd-83303834f937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e408ae6-d739-4436-8d22-5836345777e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9b93df-a496-4847-bb19-c400d540d69b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "54f80c042875442cbc4886b33a7fed1b",
      "a763b3e0cf794b81b0e1d58dd6cd2fdc",
      "b8e96e211f96493f9bafdc9a8f8083fb",
      "3b29eaf965b74f8b8a928f7e1de00a71",
      "85126b4bc6c241b0b6ac6f0fc16d2fc4",
      "ddc61dc02f8c44da9dd1ce8f3828e379",
      "ef45cbd9e7744aae938b654ecb5eac11",
      "287c0aa4ae014130afdf66c79026f284",
      "0c17481318eb4f65a2e4afef66446916",
      "1c2c78b8560d4e84a3d8a63eb90e6a9f",
      "4e6c41945a704a579e6a455bb598cc56",
      "610b9c4c9e12491cb5833d3bc3dd300a",
      "063fe5fe1af74e6489b9f265b3541e09",
      "cdc1772e03d54b2c9d48a9e668156c4d",
      "4e00c89d62044e27aa5a37d8561d9a4d",
      "207b8969abf14a028dfd650b4378dd14",
      "0efcadf24d04404fb85c4d7a8556df4b",
      "0c5dc31513864bf995bda491fb83b0cf",
      "0b5ab99e70924d4681c35cca99eb26f6",
      "7ad382ac2fee4ca3bdd53a743dd7ca9b",
      "316fc74123eb498f9b867f35c6bde185",
      "5e426aa6112940db8de9017616f159dc",
      "473f60c52bf847a8bb2279b9633f913a",
      "47fa45bebf4c4659a77b258d980e37aa",
      "2dbca1333acc4e17af43239de903bbfd",
      "8edef4788f284f9d9d240dd927e391b4",
      "346e9644dd5b476db03fb322758142e9",
      "29c2e2ec675a485d89e70d1accb93eb1",
      "95e98ef53d4a4da194a3afd7e5a4e3d4",
      "ffe1e64502f44624b8462bb8b0882e51",
      "ea75c05a7f3749ff9cc0980470d4a70b",
      "9f6413789c284a2cafb73a2b46d2e261",
      "c167041d90184f0fb47646759bda9148"
     ]
    },
    "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111",
    "outputId": "03c77fb1-b50f-416c-949b-35cb3e61b2ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def extract_input_features(sample):\n",
    "    input_features = processor(\n",
    "        sample[\"audio\"][\"array\"],\n",
    "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"distil-whisper/librispeech_long\", \"clean\", split=\"validation\"\n",
    ")\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c25625-7462-4a3b-8ca8-77c65dd0dcc9",
   "metadata": {
    "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
   },
   "source": [
    "### Running Model Inference\n",
    "\n",
    "With the input sample prepared, we can now perform speech recognition using the Whisper model. The model provides a convenient `generate` interface that simplifies the inference process. Here's how you can run the model inference:\n",
    "\n",
    "1. Pass the `input_features` tensor to the `generate` method of the Whisper model.\n",
    "2. The model will process the input and generate the predicted token IDs.\n",
    "3. Once the generation is complete, use the `processor.batch_decode` method to decode the predicted token IDs into human-readable text transcription.\n",
    "\n",
    "The `generate` method handles the complex task of sequence generation, taking into account the model's architecture and the provided input features. It produces the predicted token IDs, which represent the transcribed text in a encoded format.\n",
    "\n",
    "By leveraging the `generate` interface and the `processor.batch_decode` method, you can easily perform speech recognition with the Whisper model. The model takes care of the complex task of mapping the audio input to text output, while the processor handles the necessary decoding step to provide you with the final transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62840b-69a7-4dff-bca0-2226a28a87a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "c9618867-beae-4875-a5be-0e0a3b453414",
    "outputId": "7733e9a6-6f7a-417d-b4ee-1e99f2aec090",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "predicted_ids = pt_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "print(f\"Reference: {sample['text']}\")\n",
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb97e-3c2b-4a48-a648-1488fd30d127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08389b7-fe2e-4152-92cf-ceec0483051c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f9f12-187f-4d37-985a-3ea3b3a20fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70615b8a-bb1a-49fb-b34b-d2f6773fd612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "generate_kwargs = {\n",
    "        \"max_new_tokens\": 448,\n",
    "        \"num_beams\": 1,\n",
    "        \"condition_on_prev_tokens\": False,\n",
    "        \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n",
    "        \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "        \"logprob_threshold\": -1.0,\n",
    "        \"no_speech_threshold\": 0.6,\n",
    "        \"return_timestamps\": True,\n",
    "}\n",
    "\n",
    "result = pipe(sample, generate_kwargs=generate_kwargs)\n",
    "\n",
    "#result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ba4ad-0f4b-4f9c-b770-83d7581d78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,  # batch size for inference - set based on your device\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658623-0581-4daa-9bae-1a61e4bb8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935e377-94ca-402f-9b39-b8e2681ef0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# Enable static cache and compile the forward pass\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "# 2 warmup steps\n",
    "for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
    "        with sdpa_kernel(SDPBackend.MATH):\n",
    "                    result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n",
    "                \n",
    "# fast run\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    result = pipe(sample.copy())\n",
    "                        \n",
    "print(result[\"text\"])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db79c40-0d3e-4eee-a095-8df9ff17a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
