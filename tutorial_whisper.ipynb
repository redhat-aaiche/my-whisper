{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper models:\n",
    "* [OpenAI Whisper - installation requirements](https://github.com/openai/whisper)\n",
    "* [Hugging Face Transformers - Whisper](https://huggingface.co/openai/whisper-large-v3/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aa added\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aa: changed audio file path\n",
    "# Config params\n",
    "pipeline_id: str = \"automatic-speech-recognition\"\n",
    "model_id: str = \"openai/whisper-large-v3\"\n",
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "audio_file: str = r\"./grip_audio_file.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model and spreech processing pipeline\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate processors (which includes the tokenizer and feature extractor for Whisper)\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipe = pipeline(\n",
    "    pipeline_id,\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# play audio\n",
    "Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz -o ffmpeg.tar.xz \\\n",
    "     && tar -xf ffmpeg.tar.xz && rm ffmpeg.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffmdir = !find . -iname ffmpeg-*-static\n",
    "path = %env PATH\n",
    "path = path + ':' + ffmdir[0]\n",
    "%env PATH $path\n",
    "!which ffmpeg\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# aa: added previous %%time\n",
    "# Run transcription\n",
    "generate_kwargs = {\n",
    "    \"language\": \"german\",\n",
    "    \"task\": \"transcribe\",\n",
    "    \"condition_on_prev_tokens\": True,\n",
    "    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    \"logprob_threshold\": -1.0,\n",
    "    \"no_speech_threshold\": 0.6,\n",
    "}\n",
    "\n",
    "# run transcription \n",
    "try:\n",
    "    #aa added return_timestamps=True\n",
    "    result = pipe(audio_file, generate_kwargs=generate_kwargs, return_timestamps=True)\n",
    "\n",
    "    # play audio\n",
    "    display(Audio(audio_file))\n",
    "\n",
    "    # show transcribed text\n",
    "    text = result[\"text\"]\n",
    "    print(f\"Transcribed text:\\n\\t{text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time   \n",
    "#aa: added previous %%time\n",
    "# Run transcription\n",
    "generate_kwargs = {\n",
    "    \"language\": \"german\",\n",
    "    \"task\": \"translate\",\n",
    "    \"condition_on_prev_tokens\": True,\n",
    "    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    \"logprob_threshold\": -1.0,\n",
    "    \"no_speech_threshold\": 0.6,\n",
    "}\n",
    "\n",
    "# run transcription \n",
    "try:\n",
    "    #aa added return_timestamps=True\n",
    "    result = pipe(audio_file, generate_kwargs=generate_kwargs, return_timestamps=True)\n",
    "\n",
    "    # play audio\n",
    "    display(Audio(audio_file))\n",
    "\n",
    "    # show transcribed text\n",
    "    text = result[\"text\"]\n",
    "    print(f\"Translated text:\\n\\t{text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aa: convert the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[exporters] in /opt/app-root/lib/python3.9/site-packages (1.23.3)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (23.2)\n",
      "Requirement already satisfied: coloredlogs in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (15.0.1)\n",
      "Requirement already satisfied: transformers>=4.29 in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (4.48.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (1.12)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (1.24.4)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (0.27.1)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (2.0.1+cu118)\n",
      "Requirement already satisfied: onnx in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (1.15.0)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.13-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: onnxruntime in /opt/app-root/lib/python3.9/site-packages (from optimum[exporters]) (1.19.2)\n",
      "Collecting transformers>=4.29\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2024.2.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum[exporters]) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum[exporters]) (2.0.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum[exporters]) (3.2.1)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum[exporters]) (17.0.6)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum[exporters]) (3.28.3)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m134.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.29->optimum[exporters]) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.29->optimum[exporters]) (0.5.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/app-root/lib/python3.9/site-packages (from coloredlogs->optimum[exporters]) (10.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (15.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (3.9.3)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum[exporters]) (1.5.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/app-root/lib/python3.9/site-packages (from onnx->optimum[exporters]) (3.20.2)\n",
      "Requirement already satisfied: flatbuffers in /opt/app-root/lib/python3.9/site-packages (from onnxruntime->optimum[exporters]) (24.12.23)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->optimum[exporters]) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /opt/app-root/lib/python3.9/site-packages (from timm->optimum[exporters]) (0.15.2+cu118)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum[exporters]) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (1.26.18)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.11->optimum[exporters]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->optimum[exporters]) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->optimum[exporters]) (2024.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/app-root/lib/python3.9/site-packages (from torchvision->timm->optimum[exporters]) (10.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[exporters]) (1.16.0)\n",
      "Installing collected packages: tokenizers, transformers, timm\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.0\n",
      "    Uninstalling transformers-4.48.0:\n",
      "      Successfully uninstalled transformers-4.48.0\n",
      "Successfully installed timm-1.0.13 tokenizers-0.20.3 transformers-4.46.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[exporters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "usage: optimum-cli export onnx [-h] -m MODEL [--task TASK] [--opset OPSET]\n",
      "                               [--device DEVICE] [--fp16]\n",
      "                               [--dtype {fp32,fp16,bf16}]\n",
      "                               [--optimize {O1,O2,O3,O4}] [--monolith]\n",
      "                               [--no-post-process] [--variant VARIANT]\n",
      "                               [--framework {pt,tf}] [--atol ATOL]\n",
      "                               [--cache_dir CACHE_DIR] [--trust-remote-code]\n",
      "                               [--pad_token_id PAD_TOKEN_ID]\n",
      "                               [--library-name {transformers,diffusers,timm,sentence_transformers}]\n",
      "                               [--model-kwargs MODEL_KWARGS] [--legacy]\n",
      "                               [--no-dynamic-axes] [--no-constant-folding]\n",
      "                               [--batch_size BATCH_SIZE]\n",
      "                               [--sequence_length SEQUENCE_LENGTH]\n",
      "                               [--num_choices NUM_CHOICES] [--width WIDTH]\n",
      "                               [--height HEIGHT] [--num_channels NUM_CHANNELS]\n",
      "                               [--feature_size FEATURE_SIZE]\n",
      "                               [--nb_max_frames NB_MAX_FRAMES]\n",
      "                               [--audio_sequence_length AUDIO_SEQUENCE_LENGTH]\n",
      "                               [--point_batch_size POINT_BATCH_SIZE]\n",
      "                               [--nb_points_per_image NB_POINTS_PER_IMAGE]\n",
      "                               output\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Required arguments:\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load\n",
      "                        model from.\n",
      "  output                Path indicating the directory where to store the\n",
      "                        generated ONNX model.\n",
      "\n",
      "Optional arguments:\n",
      "  --task TASK           The task to export the model for. If not specified,\n",
      "                        the task will be auto-inferred based on the model.\n",
      "                        Available tasks depend on the model, but are among:\n",
      "                        ['image-to-text', 'semantic-segmentation', 'multiple-\n",
      "                        choice', 'zero-shot-image-classification', 'token-\n",
      "                        classification', 'text-classification', 'depth-\n",
      "                        estimation', 'text-generation', 'text2text-\n",
      "                        generation', 'feature-extraction', 'audio-\n",
      "                        classification', 'image-classification', 'sentence-\n",
      "                        similarity', 'masked-im', 'audio-frame-\n",
      "                        classification', 'zero-shot-object-detection', 'image-\n",
      "                        segmentation', 'fill-mask', 'question-answering',\n",
      "                        'automatic-speech-recognition', 'mask-generation',\n",
      "                        'text-to-audio', 'audio-xvector', 'image-to-image',\n",
      "                        'object-detection']. For decoder models, use `xxx-\n",
      "                        with-past` to export the model using past key values\n",
      "                        in the decoder.\n",
      "  --opset OPSET         If specified, ONNX opset version to export the model\n",
      "                        with. Otherwise, the default opset for the given model\n",
      "                        architecture will be used.\n",
      "  --device DEVICE       The device to use to do the export. Defaults to \"cpu\".\n",
      "  --fp16                Use half precision during the export. PyTorch-only,\n",
      "                        requires `--device cuda`.\n",
      "  --dtype {fp32,fp16,bf16}\n",
      "                        The floating point precision to use for the export.\n",
      "                        Supported options: fp32 (float32), fp16 (float16),\n",
      "                        bf16 (bfloat16).\n",
      "  --optimize {O1,O2,O3,O4}\n",
      "                        Allows to run ONNX Runtime optimizations directly\n",
      "                        during the export. Some of these optimizations are\n",
      "                        specific to ONNX Runtime, and the resulting ONNX will\n",
      "                        not be usable with other runtime as OpenVINO or\n",
      "                        TensorRT. Possible options: - O1: Basic general\n",
      "                        optimizations - O2: Basic and extended general\n",
      "                        optimizations, transformers-specific fusions - O3:\n",
      "                        Same as O2 with GELU approximation - O4: Same as O3\n",
      "                        with mixed precision (fp16, GPU-only, requires\n",
      "                        `--device cuda`)\n",
      "  --monolith            Forces to export the model as a single ONNX file. By\n",
      "                        default, the ONNX exporter may break the model in\n",
      "                        several ONNX files, for example for encoder-decoder\n",
      "                        models where the encoder should be run only once while\n",
      "                        the decoder is looped over.\n",
      "  --no-post-process     Allows to disable any post-processing done by default\n",
      "                        on the exported ONNX models. For example, the merging\n",
      "                        of decoder and decoder-with-past models into a single\n",
      "                        ONNX model file to reduce memory usage.\n",
      "  --variant VARIANT     Select a variant of the model to export.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
      "                        provided, will attempt to use the local checkpoint's\n",
      "                        original framework or what is available in the\n",
      "                        environment.\n",
      "  --atol ATOL           If specified, the absolute difference tolerance when\n",
      "                        validating the model. Otherwise, the default atol for\n",
      "                        the model will be used.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --trust-remote-code   Allows to use custom code for the modeling hosted in\n",
      "                        the model repository. This option should only be set\n",
      "                        for repositories you trust and in which you have read\n",
      "                        the code, as it will execute on your local machine\n",
      "                        arbitrary code present in the model repository.\n",
      "  --pad_token_id PAD_TOKEN_ID\n",
      "                        This is needed by some models, for some tasks. If not\n",
      "                        provided, will attempt to use the tokenizer to guess\n",
      "                        it.\n",
      "  --library-name {transformers,diffusers,timm,sentence_transformers}\n",
      "                        The library on the model. If not provided, will\n",
      "                        attempt to infer the local checkpoint's library\n",
      "  --model-kwargs MODEL_KWARGS\n",
      "                        Any kwargs passed to the model forward, or used to\n",
      "                        customize the export for a given model.\n",
      "  --legacy              Export decoder only models in three files (without +\n",
      "                        with past and the resulting merged model).Also disable\n",
      "                        the use of position_ids for text-generation models\n",
      "                        that require it for batched generation. This argument\n",
      "                        is introduced for backward compatibility and will be\n",
      "                        removed in a future release of Optimum.\n",
      "  --no-dynamic-axes     Disable dynamic axes during ONNX export\n",
      "  --no-constant-folding\n",
      "                        PyTorch-only argument. Disables PyTorch ONNX export\n",
      "                        constant folding.\n",
      "\n",
      "Input shapes (if necessary, this allows to override the shapes of the input given to the ONNX exporter, that requires an example input).:\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Text tasks only. Batch size to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --sequence_length SEQUENCE_LENGTH\n",
      "                        Text tasks only. Sequence length to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --num_choices NUM_CHOICES\n",
      "                        Text tasks only. Num choices to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --width WIDTH         Image tasks only. Width to use in the example input\n",
      "                        given to the ONNX export.\n",
      "  --height HEIGHT       Image tasks only. Height to use in the example input\n",
      "                        given to the ONNX export.\n",
      "  --num_channels NUM_CHANNELS\n",
      "                        Image tasks only. Number of channels to use in the\n",
      "                        example input given to the ONNX export.\n",
      "  --feature_size FEATURE_SIZE\n",
      "                        Audio tasks only. Feature size to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --nb_max_frames NB_MAX_FRAMES\n",
      "                        Audio tasks only. Maximum number of frames to use in\n",
      "                        the example input given to the ONNX export.\n",
      "  --audio_sequence_length AUDIO_SEQUENCE_LENGTH\n",
      "                        Audio tasks only. Audio sequence length to use in the\n",
      "                        example input given to the ONNX export.\n",
      "  --point_batch_size POINT_BATCH_SIZE\n",
      "                        For Segment Anything. It corresponds to how many\n",
      "                        segmentation masks we want the model to predict per\n",
      "                        input point.\n",
      "  --nb_points_per_image NB_POINTS_PER_IMAGE\n",
      "                        For Segment Anything. It corresponds to the number of\n",
      "                        points per segmentation masks.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p models2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!optimum-cli export onnx --model openai/whisper-large-v3 models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1017: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:334: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1477: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:458: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:443: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
      "\t\t-[x] values not close enough, max diff: 0.002777099609375 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 7.021236419677734 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 4.966104030609131 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 2.263120174407959 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.432504653930664 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 0.7931453585624695 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 3.5463569164276123 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 2.014420986175537 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 12.685822486877441 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.2551497220993042 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 6.91324520111084 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.0228071212768555 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 7.65731143951416 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.6308841705322266 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 7.0712080001831055 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 0.9848865270614624 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 5.34073543548584 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.0451489686965942 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 5.886284351348877 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 0.978852391242981 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 4.860201835632324 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.1861058473587036 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 3.133962631225586 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.8104567527770996 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 2.6916275024414062 (atol: 0.001)\n",
      "\t\t-[x] values not close enough, max diff: 1.8608214855194092 (atol: 0.001)\n",
      "Validation for the model models2/encoder_model.onnx raised: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 0.001:\n",
      "- last_hidden_state: max diff = 0.002777099609375\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 0.001:\n",
      "- logits: max diff = 7.021236419677734\n",
      "- present.0.decoder.key: max diff = 4.966104030609131\n",
      "- present.0.decoder.value: max diff = 2.263120174407959\n",
      "- present.1.decoder.key: max diff = 1.432504653930664\n",
      "- present.1.decoder.value: max diff = 0.7931453585624695\n",
      "- present.2.decoder.key: max diff = 3.5463569164276123\n",
      "- present.2.decoder.value: max diff = 2.014420986175537\n",
      "- present.3.decoder.key: max diff = 12.685822486877441\n",
      "- present.3.decoder.value: max diff = 1.2551497220993042\n",
      "- present.4.decoder.key: max diff = 6.91324520111084\n",
      "- present.4.decoder.value: max diff = 1.0228071212768555\n",
      "- present.5.decoder.key: max diff = 7.65731143951416\n",
      "- present.5.decoder.value: max diff = 1.6308841705322266\n",
      "- present.6.decoder.key: max diff = 7.0712080001831055\n",
      "- present.6.decoder.value: max diff = 0.9848865270614624\n",
      "- present.7.decoder.key: max diff = 5.34073543548584\n",
      "- present.7.decoder.value: max diff = 1.0451489686965942\n",
      "- present.8.decoder.key: max diff = 5.886284351348877\n",
      "- present.8.decoder.value: max diff = 0.978852391242981\n",
      "- present.9.decoder.key: max diff = 4.860201835632324\n",
      "- present.9.decoder.value: max diff = 1.1861058473587036\n",
      "- present.10.decoder.key: max diff = 3.133962631225586\n",
      "- present.10.decoder.value: max diff = 1.8104567527770996\n",
      "- present.11.decoder.key: max diff = 2.6916275024414062\n",
      "- present.11.decoder.value: max diff = 1.8608214855194092.\n",
      " The exported model was saved at: models2\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model openai/whisper-small  models2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#not needed pytorch image has already these packages\n",
    "#!pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "if not all([aws_access_key_id, aws_secret_access_key, endpoint_url, region_name, bucket_name]):\n",
    "   raise ValueError(\"One or more connection variables are empty.  \"\n",
    "                    \"Please check your connection to an S3 bucket.\")\n",
    "        \n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "        \n",
    "s3_resource = session.resource(\n",
    "   's3',\n",
    "   config=botocore.client.Config(signature_version='s3v4'),\n",
    "   endpoint_url=endpoint_url,\n",
    "   region_name=region_name)\n",
    "        \n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "        \n",
    "        \n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "  num_files = 0\n",
    "  for root, dirs, files in os.walk(local_directory):\n",
    "     for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        relative_path = os.path.relpath(file_path, local_directory)\n",
    "        s3_key = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"{file_path} -> {s3_key}\")\n",
    "        bucket.upload_file(file_path, s3_key)\n",
    "        num_files += 1\n",
    "  return num_files\n",
    "                                                        \n",
    "                                                        \n",
    "def list_objects(prefix):\n",
    "   filter = bucket.objects.filter(Prefix=prefix)\n",
    "   for obj in filter.all():\n",
    "      print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/config.json -> models/config.json\n",
      "models/generation_config.json -> models/generation_config.json\n",
      "models/tokenizer_config.json -> models/tokenizer_config.json\n",
      "models/special_tokens_map.json -> models/special_tokens_map.json\n",
      "models/added_tokens.json -> models/added_tokens.json\n",
      "models/vocab.json -> models/vocab.json\n",
      "models/merges.txt -> models/merges.txt\n",
      "models/normalizer.json -> models/normalizer.json\n",
      "models/tokenizer.json -> models/tokenizer.json\n",
      "models/preprocessor_config.json -> models/preprocessor_config.json\n",
      "models/encoder_model.onnx -> models/encoder_model.onnx\n",
      "models/decoder_model.onnx -> models/decoder_model.onnx\n",
      "models/decoder_with_past_model.onnx -> models/decoder_with_past_model.onnx\n",
      "models/decoder_model_merged.onnx -> models/decoder_model_merged.onnx\n"
     ]
    }
   ],
   "source": [
    "local_models_directory = \"models\"\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "   raise ValueError(f\"The directory '{local_models_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "        \n",
    "num_files = upload_directory_to_s3(\"models\", \"models\")\n",
    "        \n",
    "if num_files == 0:\n",
    "   raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                    \"saving the model to the \\\"models\\\" directory?  \"\n",
    "                    \"Check for \\\"models/fraud/1/model.onnx\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/added_tokens.json\n",
      "models/config.json\n",
      "models/decoder_model.onnx\n",
      "models/decoder_model_merged.onnx\n",
      "models/decoder_with_past_model.onnx\n",
      "models/encoder_model.onnx\n",
      "models/generation_config.json\n",
      "models/merges.txt\n",
      "models/normalizer.json\n",
      "models/preprocessor_config.json\n",
      "models/special_tokens_map.json\n",
      "models/tokenizer.json\n",
      "models/tokenizer_config.json\n",
      "models/vocab.json\n"
     ]
    }
   ],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -s https://cc-cc.apps.ocp4.example.com/v2/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "hf_model_id = \"openai/whisper-medium\" # Change to your model url\n",
    "hf_model = WhisperForConditionalGeneration.from_pretrained(hf_model_id)\n",
    "torch.save(merged_model, \"./pretrained_model_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in /opt/app-root/lib/python3.9/site-packages (1.23.3)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.48.0)\n",
      "Requirement already satisfied: onnxruntime in /opt/app-root/lib/python3.9/site-packages (1.19.2)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from optimum) (23.2)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/app-root/lib/python3.9/site-packages (from optimum) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/app-root/lib/python3.9/site-packages (from optimum) (0.27.1)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib/python3.9/site-packages (from optimum) (3.2.0)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from optimum) (1.24.4)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: coloredlogs in /opt/app-root/lib/python3.9/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: protobuf in /opt/app-root/lib/python3.9/site-packages (from onnxruntime) (3.20.2)\n",
      "Requirement already satisfied: flatbuffers in /opt/app-root/lib/python3.9/site-packages (from onnxruntime) (24.12.23)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.11->optimum) (3.1.3)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum) (17.0.6)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum) (3.28.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/app-root/lib/python3.9/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (3.9.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->optimum) (15.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum transformers onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\"\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "model = ORTModelForSpeechSeq2Seq.from_pretrained('onnx_variant', export=True)\n",
    "model.save_pretrained('./onnx_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model_name = \"openai/whisper-medium\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./onnx_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "if not all([aws_access_key_id, aws_secret_access_key, endpoint_url, region_name, bucket_name]):\n",
    "   raise ValueError(\"One or more connection variables are empty.  \"\n",
    "                    \"Please check your connection to an S3 bucket.\")\n",
    "        \n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "        \n",
    "s3_resource = session.resource(\n",
    "   's3',\n",
    "   config=botocore.client.Config(signature_version='s3v4'),\n",
    "   endpoint_url=endpoint_url,\n",
    "   region_name=region_name)\n",
    "        \n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "        \n",
    "        \n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "  num_files = 0\n",
    "  for root, dirs, files in os.walk(local_directory):\n",
    "     for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        relative_path = os.path.relpath(file_path, local_directory)\n",
    "        s3_key = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"{file_path} -> {s3_key}\")\n",
    "        bucket.upload_file(file_path, s3_key)\n",
    "        num_files += 1\n",
    "  return num_files\n",
    "                                                        \n",
    "                                                        \n",
    "def list_objects(prefix):\n",
    "   filter = bucket.objects.filter(Prefix=prefix)\n",
    "   for obj in filter.all():\n",
    "      print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/config.json -> models/config.json\n",
      "models/generation_config.json -> models/generation_config.json\n",
      "models/tokenizer_config.json -> models/tokenizer_config.json\n",
      "models/special_tokens_map.json -> models/special_tokens_map.json\n",
      "models/added_tokens.json -> models/added_tokens.json\n",
      "models/vocab.json -> models/vocab.json\n",
      "models/merges.txt -> models/merges.txt\n",
      "models/normalizer.json -> models/normalizer.json\n",
      "models/tokenizer.json -> models/tokenizer.json\n",
      "models/preprocessor_config.json -> models/preprocessor_config.json\n",
      "models/encoder_model.onnx -> models/encoder_model.onnx\n",
      "models/decoder_model.onnx -> models/decoder_model.onnx\n"
     ]
    }
   ],
   "source": [
    "local_models_directory = \"models\"\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "   raise ValueError(f\"The directory '{local_models_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "        \n",
    "num_files = upload_directory_to_s3(\"models\", \"models\")\n",
    "        \n",
    "if num_files == 0:\n",
    "   raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                    \"saving the model to the \\\"models\\\" directory?  \"\n",
    "                    \"Check for \\\"models/fraud/1/model.onnx\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model as ONNX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b6a830f4514ba9b0f317f590343d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff47b6c1519415f8f7d38499f2a00ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c75603679f442595fd1b9b34f747a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df6bab653164c3f886bcbf1b674ea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ce705cd334753a0ad067abf81ad11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fc4e5b84d3496088b7e0619bbdfb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f667237085426daf4587152628a472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15fd870c048497cbf6bd674d4232a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141a702beb274ca59da6b86dbd3d6b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbb5037138a45359f7f89203b4d706c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6074ca162904f2cbd727465221748b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1016: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:334: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1476: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
      "\t\t-[x] values not close enough, max diff: 0.01586604118347168 (atol: 0.001)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 0.001:\n",
      "- last_hidden_state: max diff = 0.01586604118347168.\n",
      " The exported model was saved at: models\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperConfig\n",
    "from optimum.exporters.onnx import main_export\n",
    "from optimum.exporters.onnx.model_configs import WhisperOnnxConfig\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "print(\"Exporting model as ONNX\")\n",
    "\n",
    "config = WhisperConfig.from_pretrained(model_id)\n",
    "onnx_config = WhisperOnnxConfig(config, task=\"automatic-speech-recognition\")\n",
    "\n",
    "encoder_config = onnx_config.with_behavior(\"encoder\")\n",
    "decoder_config = onnx_config.with_behavior(\"decoder\")\n",
    "\n",
    "custom_onnx_configs={\n",
    "        \"encoder_model\": encoder_config,\n",
    "        \"decoder_model\": decoder_config\n",
    "}\n",
    "\n",
    "main_export(\n",
    "        model_id,\n",
    "        output=\"models\",\n",
    "        task=\"automatic-speech-recognition\",\n",
    "        custom_onnx_configs=custom_onnx_configs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
