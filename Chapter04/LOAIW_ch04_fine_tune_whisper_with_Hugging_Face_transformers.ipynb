{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d61b04e-4228-4935-b078-e87467266f91",
   "metadata": {
    "tags": []
   },
   "source": [
    "Greetings to :\n",
    "    https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
   "metadata": {
    "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
   },
   "source": [
    "# Learn OpenAI Whisper - Chapter 4\n",
    "## Fine-tune Whisper with Hugging Face transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb8d21-df06-472a-99dd-b59567be6dad",
   "metadata": {
    "id": "55fb8d21-df06-472a-99dd-b59567be6dad"
   },
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a",
   "metadata": {
    "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a"
   },
   "source": [
    "We can verify that we've been assigned a GPU and view its specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4acc004-8994-46bf-8eaf-6ba981df100d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aa: comment - pytorch doesnt have the tool"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a49fd81a-788a-46b2-a47a-91518e4f6c68",
   "metadata": {
    "id": "95048026-a3b7-43f0-a274-1bad65e407b4",
    "tags": []
   },
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbf1e39-3d28-4043-b174-9e63af312e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.0.1+cu118 (cpu)\n"
     ]
    }
   ],
   "source": [
    "# aa: added this cell\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d",
   "metadata": {
    "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d"
   },
   "source": [
    "We'll employ several popular Python packages to fine-tune the Whisper model.\n",
    "We'll use `datasets` to download and prepare our training data and\n",
    "`transformers` to load and train our Whisper model. We'll also require\n",
    "the `soundfile` package to pre-process audio files, `evaluate` and `jiwer` to\n",
    "assess the performance of our model. Finally, we'll\n",
    "use `gradio` to build a flashy demo of our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
   "metadata": {
    "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#aa: needed to run twice the following cell\n",
    "\n",
    "!pip install -q datasets>=2.6.1\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q librosa\n",
    "!pip install -q evaluate>=0.30\n",
    "!pip install -q jiwer\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "J9wGvNnjmg4I",
   "metadata": {
    "id": "J9wGvNnjmg4I",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203",
   "metadata": {
    "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203"
   },
   "source": [
    "We highly recommend directly uploading model checkpoints to the [Hugging Face Hub](https://huggingface.co/) while you train. The Hub offers several benefits:\n",
    "- **Version Control**: Ensures every model checkpoint is preserved throughout the training process.\n",
    "- **Tensorboard Logs**: Allows for the monitoring of key metrics as training progresses.\n",
    "- **Model Cards**: Provides a space to explain the model's functionality and its appropriate applications.\n",
    "- **Community Engagement**: Facilitates sharing and collaboration with a wider community.\n",
    "\n",
    "Connecting this notebook with the Hub is quite simple – all it requires is your Hub authentication token when asked. You can locate your Hub authentication token [here](https://huggingface.co/settings/tokens).\n",
    "\n",
    "**IMPORTANT**\n",
    "Make sure that your Hugging Face token was created with role `write` because the notebook will be saving the model to the Hugging Face repository.\n",
    "\n",
    "If you need instructions on how to create a Hugging Face tocken, please check out the [notebook for Chapter 3](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea947fb-11fe-416f-ae56-d8b01a88db91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4fccd7-73fb-43b4-b792-c42d91279f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/app-root/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface_hub) (3.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
   "metadata": {
    "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd234bf08f4cde9aecc0edaedbcae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5NgaNa3RjfNU",
   "metadata": {
    "id": "5NgaNa3RjfNU"
   },
   "source": [
    "**IMPORTANT:**\n",
    "The output of `whoami90` shoud show `'role': 'write'` in your `auth` keys.. If it does not, then you might need to create a new token with role `write`, rerun the previous cell to execute `notebook_login()`, and load the new token.  \n",
    "\n",
    "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter04/hugging_face_whoami.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "t-Cj-HImlUSd",
   "metadata": {
    "id": "t-Cj-HImlUSd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '676187706ce06148154eb2f8',\n",
       " 'name': 'aaiche',\n",
       " 'fullname': 'aaiche',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/1bf3857a4d2a4e5e6fb5381ba1ed7752.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'AA_TOKEN',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-01-11T15:38:55.885Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '676187706ce06148154eb2f8',\n",
       "       'type': 'user',\n",
       "       'name': 'aaiche'},\n",
       "      'permissions': ['repo.content.read']}]}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
   "metadata": {
    "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674429c5-0ab4-4adf-975b-621bb69eca38",
   "metadata": {
    "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
   },
   "source": [
    "Utilizing 🤗 Datasets makes the task of downloading and preparing data remarkably straightforward.\n",
    "You can obtain and ready the Common Voice splits with a mere single line of code.\n",
    "\n",
    "Initially, confirm that you've agreed to the usage terms on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). After agreeing to the terms, you gain complete access to the dataset, enabling you to download the data to your local environment.\n",
    "\n",
    "Given the scarcity of resources for Hindi, we plan to merge the `train` and `validation` splits, resulting in around 8 hours of training material. The 4 hours of `test` data will serve as our separate test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b75a44-0fc0-477a-9b95-f8f3a305f3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/whisper/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/whisper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eba7112-7b3d-4d2f-b87e-710055195c83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x. 2 1000740000 root 40 Jan 14 13:02 .\n",
      "drwxr-xr-x. 6 1000740000 root 61 Jan 14 13:02 ..\n",
      "-rw-r--r--. 1 1000740000 root 61 Jan 14 13:02 stored_tokens\n",
      "-rw-r--r--. 1 1000740000 root 37 Jan 14 13:02 token\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c56d9d-db76-41fe-b852-dcc09b937d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/huggingface/datasets': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01c402-dee6-4359-82f5-f66e6d7d0498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
   "metadata": {
    "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f9c0cab7c645a8b24677a322aca046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8371feee90e24195ad91cb55939fb415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "common_voice_11_0.py:   0%|          | 0.00/8.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca7a4e1bd2c43abaa2fbee7b6d875fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "languages.py:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c907fe66ad4308b865bb8ab5789b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "release_stats.py:   0%|          | 0.00/60.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40e06a58e424f2eb8f1792cfb728c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_shards.json:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a9500fadcc4cd98f29bb3b56b293af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hi_train_0.tar:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6432c65a83d5466eb2df9a1ac3e4f173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hi_dev_0.tar:   0%|          | 0.00/61.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8c8eb91015442e8710471dfc06653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hi_test_0.tar:   0%|          | 0.00/92.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32a3a7cece648e1917061dea51990e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hi_other_0.tar:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937c9606a600492d9973d6b982f9e39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hi_invalidated_0.tar:   0%|          | 0.00/23.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/tarfile.py:2239: RuntimeWarning: The default behavior of tarfile extraction has been changed to disallow common exploits (including CVE-2007-4559). By default, absolute/parent paths are disallowed and some mode bits are cleared. See https://access.redhat.com/articles/7004769 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52fd0c8e2754c48987758c57e689587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.tsv:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c54d5dcf52644159deed77f2097ef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.tsv:   0%|          | 0.00/627k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947a142676b94e6aac7edf2e860627d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/824k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc5e20f9d444bc0a2a37830aaf37f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "other.tsv:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2158a368c2a64f8fa83299ba9dcf7208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "invalidated.tsv:   0%|          | 0.00/201k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42f75278b9c49f489dca2a687a59749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 4361it [00:00, 77105.53it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be9cc77cab74f65a62ef65d47fd0cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 2179it [00:00, 105063.73it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f8c5ca07394aca99384710b21514f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 2894it [00:00, 95620.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174dfee3f3f647a2b56473c6896c8b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 3328it [00:00, 125702.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae3aae05e094197b51a7858017ed9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 680it [00:00, 106841.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# aa: removed use_auth_token\n",
    "#common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", use_auth_token=True)\n",
    "#common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "# aa: removed use_auth_token\n",
    "# aa: added trust_remote_code=True to avoid being prompted\n",
    "# aa: tried to load fr : gave up as it took a while to load the fr dataset\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", trust_remote_code=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", trust_remote_code=True)\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc680d6-d3df-4f83-a8ee-0bc55e3d966d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/opt/app-root/src/.cache/whisper/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/whisper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb58e5b-dadf-4320-8588-3e5f19170fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x. 5 1000740000 root   82 Jan 14 13:02 .\n",
      "drwxr-xr-x. 6 1000740000 root   61 Jan 14 13:02 ..\n",
      "drwxr-xr-x. 4 1000740000 root 4.0K Jan 14 13:02 datasets\n",
      "drwxr-xr-x. 4 1000740000 root   75 Jan 14 13:02 hub\n",
      "drwxr-xr-x. 3 1000740000 root   49 Jan 14 13:02 modules\n",
      "-rw-r--r--. 1 1000740000 root   61 Jan 14 13:02 stored_tokens\n",
      "-rw-r--r--. 1 1000740000 root   37 Jan 14 13:02 token\n"
     ]
    }
   ],
   "source": [
    "!ls -alh  ~/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3674633-abf0-4d86-9f0a-03e89c50bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x. 4 1000740000 root 4.0K Jan 14 13:02 .\n",
      "drwxr-xr-x. 5 1000740000 root   82 Jan 14 13:02 ..\n",
      "drwxr-xr-x. 3 1000740000 root   23 Jan 14 13:02 downloads\n",
      "drwxr-xr-x. 3 1000740000 root   16 Jan 14 13:02 mozilla-foundation___common_voice_11_0\n",
      "-rw-r--r--. 1 1000740000 root    0 Jan 14 13:02 _opt_app-root_src_.cache_huggingface_datasets_mozilla-foundation___common_voice_11_0_hi_11.0.0_3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631.lock\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f",
   "metadata": {
    "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f"
   },
   "source": [
    "The majority of ASR datasets offer just the input audio clips (`audio`) and their transcribed text (`sentence`). However, Common Voice includes extra metadata, like `accent` and `locale`, which aren't necessary for ASR tasks. To maintain the notebook's broad applicability, we focus solely on the input audio and the transcribed text for fine-tuning, omitting the extra metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
   "metadata": {
    "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
   "metadata": {
    "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
   },
   "source": [
    "## Prepare feature extractor, tokenizer and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c3099-1026-439e-93e2-5635b3ba5a73",
   "metadata": {
    "id": "601c3099-1026-439e-93e2-5635b3ba5a73"
   },
   "source": [
    "The ASR process can be broken down into three main components:\n",
    "1. A feature extractor that prepares the raw audio inputs\n",
    "2. The model that executes the sequence-to-sequence transformation\n",
    "3. A tokenizer that converts the model's outputs into text format\n",
    "\n",
    "Within 🤗 Transformers, the Whisper model comes with its own feature extractor and tokenizer, named [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor) and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer), respectively.\n",
    "\n",
    "Let's dive into the specifics of configuring both the feature extractor and tokenizer step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560332eb-3558-41a1-b500-e83a9f695f84",
   "metadata": {
    "id": "560332eb-3558-41a1-b500-e83a9f695f84"
   },
   "source": [
    "### Load WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365",
   "metadata": {
    "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365"
   },
   "source": [
    "The Whisper feature extractor conducts two key tasks:\n",
    "1. Adjusts the length of the audio inputs to 30 seconds: it extends any audio shorter than 30 seconds with silence (zeros) to reach 30 seconds, and it cuts down any audio longer than 30 seconds to the same duration.\n",
    "2. Transforms the audio inputs into _log-Mel spectrogram_ input features, which visually represent the audio and match the input format the Whisper model anticipates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d9ec1-d12b-4b64-93f7-04c63997da19",
   "metadata": {
    "id": "589d9ec1-d12b-4b64-93f7-04c63997da19"
   },
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Figure 2:</b> Conversion of sampled audio array to log-Mel spectrogram.\n",
    "Left: sampled 1-dimensional audio signal. Right: corresponding log-Mel spectrogram. Figure source:\n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa",
   "metadata": {
    "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa"
   },
   "source": [
    "We'll load the feature extractor from the pre-trained checkpoint with the default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
   "metadata": {
    "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab137207af14dbc94280d2df4b3c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdefa8ddb242447983ec154bd80a8dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb",
   "metadata": {
    "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb"
   },
   "source": [
    "### Load WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc82609-a9fb-447a-a2af-99597c864029",
   "metadata": {
    "id": "2bc82609-a9fb-447a-a2af-99597c864029"
   },
   "source": [
    "The Whisper model generates a series of _token ids_. Each token id is then translated into its respective text string by the tokenizer. For the Hindi language, we are able to utilize the pre-trained tokenizer directly for fine-tuning without needing any adjustments. All that's required is to define the target language and the task at hand. This instruction guides the tokenizer to prepend the language and task tokens at the beginning of the encoded label sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
   "metadata": {
    "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e939ab9cd946c48844a1abc068a304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aec95c9b0040b88fa3c418be739002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6564138797754aa9af629b9729c07b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4770d387b267403e87633fad4b7b2737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb1138d6d56461aa084e1e6d74cf0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1010e01e5743e4a3c2ef761fd48167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792617528c1e45129c7daf2d45b1ead6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b",
   "metadata": {
    "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
   },
   "source": [
    "### Combine to create a WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d",
   "metadata": {
    "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d"
   },
   "source": [
    "To make the use of the feature extractor and tokenizer more straightforward, we can bundle them together into a unified `WhisperProcessor` class. This processor amalgamates functionalities from both the `WhisperFeatureExtractor` and `WhisperTokenizer`, allowing it to be applied to both the audio inputs and model outputs as needed. By adopting this approach, we simplify the components we need to manage during training to just two items: the `processor` and the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
   "metadata": {
    "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
   "metadata": {
    "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f",
   "metadata": {
    "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
   },
   "source": [
    "Let's print the first example of the Common Voice dataset to see\n",
    "what form the data is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
   "metadata": {
    "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/opt/app-root/src/.cache/huggingface/datasets/downloads/extracted/dee614c2a82bc4300a2aacac2ba79ff9e8318fe2ded9d21aae44934f8c4a9bb9/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 5.81611368e-26, -1.48634016e-25, -9.37040538e-26, ...,\n",
      "        1.06425901e-07,  4.46416450e-08,  2.61450239e-09]), 'sampling_rate': 48000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
   "metadata": {
    "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
   },
   "source": [
    "Given that our input audio has a sampling rate of 48kHz, it's essential to _downsample_ it to 16kHz before feeding it into the Whisper feature extractor, since 16kHz is the sampling rate the Whisper model expects.\n",
    "\n",
    "We will adjust the audio inputs to the appropriate sampling rate using the dataset's [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) function. This process doesn't alter the audio files directly. Instead, it instructs the `datasets` library to resample the audio samples _on the fly_ upon their initial access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
   "metadata": {
    "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
   "metadata": {
    "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
   },
   "source": [
    "Re-loading the first audio sample in the Common Voice dataset will resample\n",
    "it to the desired sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87122d71-289a-466a-afcf-fa354b18946b",
   "metadata": {
    "id": "87122d71-289a-466a-afcf-fa354b18946b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/opt/app-root/src/.cache/huggingface/datasets/downloads/extracted/dee614c2a82bc4300a2aacac2ba79ff9e8318fe2ded9d21aae44934f8c4a9bb9/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 3.81639165e-17,  2.42861287e-17, -1.73472348e-17, ...,\n",
      "       -1.30981789e-07,  2.63096808e-07,  4.77157300e-08]), 'sampling_rate': 16000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
   "metadata": {
    "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
   },
   "source": [
    "Now, let's craft a function to get our data set for the model:\n",
    "1. By invoking `batch[\"audio\"]`, we load and resample the audio data. As mentioned before, 🤗 Datasets handles the resampling as needed automatically.\n",
    "2. The feature extractor is utilized to derive the log-Mel spectrogram input features from our one-dimensional audio array.\n",
    "3. The transcriptions are converted into label ids by employing the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6525c478-8962-4394-a1c4-103c54cce170",
   "metadata": {
    "id": "6525c478-8962-4394-a1c4-103c54cce170",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
   "metadata": {
    "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
   },
   "source": [
    "We're able to implement the data preparation function across all training examples by utilizing the dataset's `.map` method. The `num_proc` parameter determines the number of CPU cores employed. Configuring `num_proc` to be greater than 1 activates multiprocessing. Should the `.map` method experience delays while using multiprocessing, adjusting `num_proc` to 1 will ensure the dataset is processed in a sequential manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
   "metadata": {
    "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d977f259464f4e7f869f0cb0b1502aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/6540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5ad7eb88ba4367964fb3183ec02f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
   "metadata": {
    "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
   },
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7",
   "metadata": {
    "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7"
   },
   "source": [
    "With our data now prepared, we're set to embark on setting up the training pipeline. The [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) simplifies much of this process for us. Our tasks include:\n",
    "\n",
    "- Establishing a data collator: This function gathers our processed data and organizes it into PyTorch tensors, making them ready for the model.\n",
    "\n",
    "- Choosing evaluation metrics: For evaluation purposes, the model's performance will be assessed using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We'll need a `compute_metrics` function that will manage this evaluation.\n",
    "\n",
    "- Loading a pre-trained checkpoint: A pre-trained checkpoint is loaded and set up for the training phase.\n",
    "\n",
    "- Configuring the training setup: The 🤗 Trainer relies on this configuration to outline the training protocol.\n",
    "\n",
    "After fine-tuning the model, we'll assess its performance on the test dataset to ensure it's been properly trained for speech transcription in Hindi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
   "metadata": {
    "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
   },
   "source": [
    "### Define a data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04def221-0637-4a69-b242-d3f0c1d0ee78",
   "metadata": {
    "id": "04def221-0637-4a69-b242-d3f0c1d0ee78"
   },
   "source": [
    "The data collator for a sequence-to-sequence speech model distinctively processes `input_features` and `labels` separately: `input_features` are managed with the feature extractor while `labels` are dealt with by the tokenizer.\n",
    "\n",
    "`Input_features` get padded to 30 seconds and transformed into a log-Mel spectrogram of a uniform size through the feature extractor's intervention. The next step involves converting these `input_features` into batched PyTorch tensors, accomplished using the feature extractor's `.pad` method and setting `return_tensors=pt`.\n",
    "\n",
    "On the flip side, `labels` remain un-padded. The initial action is to pad these sequences to the batch's maximal length via the tokenizer's `.pad` method. Subsequently, padding tokens are substituted with `-100` to ensure they are excluded from loss calculations. Additionally, we remove the BOS token from the start of the label sequence since it will be added later in the training phase.\n",
    "\n",
    "The `WhisperProcessor` we previously established can be utilized to conduct both feature extraction and tokenization tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
   "metadata": {
    "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
   "metadata": {
    "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
   },
   "source": [
    "Let's initialise the data collator we've just defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
   "metadata": {
    "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "zaDrQSTcnX48",
   "metadata": {
    "id": "zaDrQSTcnX48",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 6540\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
   "metadata": {
    "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
   },
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fee1a7-a44c-461e-b047-c3917221572e",
   "metadata": {
    "id": "66fee1a7-a44c-461e-b047-c3917221572e"
   },
   "source": [
    "We'll employ the word error rate (WER) metric, widely considered the standard for evaluating ASR systems. For additional details, you can consult the WER [documentation](https://huggingface.co/metrics/wer). The WER metric will be sourced from 🤗 Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b22b4011-f31f-4b57-b684-c52332f92890",
   "metadata": {
    "id": "b22b4011-f31f-4b57-b684-c52332f92890",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/app-root/lib64/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f99a905f8d546839b932afbd5c0bb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508",
   "metadata": {
    "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508"
   },
   "source": [
    "We need to craft a function that processes our model predictions and calculates the WER metric. This function, named `compute_metrics`, initially substitutes `-100` with the `pad_token_id` in the `label_ids`, reversing the adjustment made in the data collator to accurately exclude padded tokens from the loss calculation. Subsequently, it translates the predicted and label ids into strings. Ultimately, it determines the WER by comparing the predictions with the reference labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
   "metadata": {
    "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a825-6d9f-4a23-b145-c37c0039075b",
   "metadata": {
    "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
   },
   "source": [
    "### Load a pre-trained checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
   "metadata": {
    "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
   },
   "source": [
    "Now, we'll proceed to load the pre-trained Whisper `small` checkpoint, which is straightforward with the use of 🤗 Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
   "metadata": {
    "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af18c2d9431747c49d0a7d27804b2158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76213bd171934a399b08c8acb78267e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ca4234ecb74fecb7b4924e6b679531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"hi\" # Define the language of your choice here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ead5f-2277-4a39-937b-585c2497b2df",
   "metadata": {
    "id": "a15ead5f-2277-4a39-937b-585c2497b2df"
   },
   "source": [
    "Adjust generation parameters - no tokens are predetermined as decoder outputs (refer to [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), and no tokens are excluded during the generation process (see [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
   "metadata": {
    "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
   "metadata": {
    "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
   },
   "source": [
    "### Define the training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
   "metadata": {
    "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
   },
   "source": [
    "For the concluding step, we specify every parameter associated with the training process. To dive deeper into the specifics of training arguments, consult the Seq2SeqTrainingArguments [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "M1gYmQVX28CO",
   "metadata": {
    "id": "M1gYmQVX28CO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '676187706ce06148154eb2f8',\n",
       " 'name': 'aaiche',\n",
       " 'fullname': 'aaiche',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/1bf3857a4d2a4e5e6fb5381ba1ed7752.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'AA_TOKEN',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-01-11T15:38:55.885Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '676187706ce06148154eb2f8',\n",
       "       'type': 'user',\n",
       "       'name': 'aaiche'},\n",
       "      'permissions': ['repo.content.read']}]}}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "whoami()\n",
    "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b69bd74-70c8-45f6-b68e-19b703cdf913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aa: changed fp16=True to fp16=False as I am only using CPU !!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
   "metadata": {
    "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=5e-5,\n",
    "    # warmup_steps=500,\n",
    "    # max_steps=4000,\n",
    "    # warmup_steps=50,\n",
    "    # max_steps=400,\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,\n",
    "    #num_train_epochs = 2.0,\n",
    "    gradient_checkpointing=True,\n",
    "    #fp16=True,\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=10,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    # save_steps=1000,\n",
    "    # eval_steps=1000,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    hub_model_id = \"aaiche/test-small-hindi\", # Change this to your own Hugging Face repo\n",
    "    # hub_token=\"\n",
    "    #push_to_hub=True,\n",
    "    push_to_hub=False,      # aa: don't push to huggingface - this is used for education\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a944d8-3112-4552-82a0-be25988b3857",
   "metadata": {
    "id": "b3a944d8-3112-4552-82a0-be25988b3857"
   },
   "source": [
    "**Note**: if one does not want to upload the model checkpoints to the Hub,\n",
    "set `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29114-d226-4f54-97cf-8718c9f94e1e",
   "metadata": {
    "id": "bac29114-d226-4f54-97cf-8718c9f94e1e"
   },
   "source": [
    "We can pass the training arguments to the 🤗 Trainer, together with our model, dataset, data collator, and `compute_metrics` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d546d7fe-0543-479a-b708-2ebabec19493",
   "metadata": {
    "id": "d546d7fe-0543-479a-b708-2ebabec19493",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/240849381.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uOrRhDGtN5S4",
   "metadata": {
    "id": "uOrRhDGtN5S4"
   },
   "source": [
    "Before initiating the training process, we'll save the processor object. Given that the processor is non-trainable, it will remain unchanged throughout the training period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "-2zQwMfEOBJq",
   "metadata": {
    "id": "-2zQwMfEOBJq",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
   "metadata": {
    "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112",
   "metadata": {
    "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112"
   },
   "source": [
    "The training duration is estimated to be around 5-10 hours, contingent on the capabilities of your GPU or the one provided by this Google Colab. If you're utilizing this Google Colab for fine-tuning a Whisper model, it's crucial to ensure that the training process does not get halted due to periods of inactivity. To circumvent this issue, you can employ a straightforward solution by inserting the following script into the console of this browser tab (by right-clicking -> selecting _inspect_ -> navigating to the _Console_ tab -> pasting the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af",
   "metadata": {
    "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af"
   },
   "source": [
    "```javascript\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\");\n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
    "}\n",
    "setInterval(ConnectButton, 60000);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55168b-2f46-4678-afa0-ff22257ec06d",
   "metadata": {
    "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
   },
   "source": [
    "The maximum GPU memory usage for this training setup is roughly 15.8GB. If the GPU assigned to your Google Colab session is not sufficient, you may run into a CUDA `\"out-of-memory\"` error upon initiating the training. Should this occur, a viable strategy is to gradually decrease the `per_device_train_batch_size` by halves and utilize [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps) as a balancing measure.\n",
    "\n",
    "To begin the training process, just run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69405a3-b232-49b8-8c2a-5620707b879b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44b63238-626e-454b-ba85-c932d0b842de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aa: comment - pytorch doesnt have the tool"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dc47325-c063-4cbc-94e8-23b84d42de87",
   "metadata": {
    "id": "PFnWSLUQocDL",
    "tags": []
   },
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "Vyt-Ene4yh-J",
   "metadata": {
    "id": "Vyt-Ene4yh-J",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "locale.getpreferredencoding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
   "metadata": {
    "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 4:49:36, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.362606</td>\n",
       "      <td>43.680691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed language=hi, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=hi.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/modeling_utils.py:2760: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.49689335584640504, metrics={'train_runtime': 17463.5699, 'train_samples_per_second': 0.092, 'train_steps_per_second': 0.006, 'total_flos': 4.61736640512e+17, 'train_loss': 0.49689335584640504, 'epoch': 0.24449877750611246})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aa expect 5 HOURS on CPU -Medium Workbench\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4360d-5721-426e-b6ac-178f833fedeb",
   "metadata": {
    "id": "34d4360d-5721-426e-b6ac-178f833fedeb"
   },
   "source": [
    "## Building a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xX3NOiTJVbqZ",
   "metadata": {
    "id": "xX3NOiTJVbqZ"
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1_qasqaAKHPR",
   "metadata": {
    "id": "1_qasqaAKHPR"
   },
   "source": [
    "# Gratitude\n",
    "\n",
    "Many thanks to Naval Katoch for his valuable technical insights. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, check out the pioneering work by Hugging Face's Sanchit Gandhi: [blog post](https://huggingface.co/blog/fine-tune-whisper)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
